# AgentOS 3.0 - System Prompt Implementation Guide V2
## Practical Integration with Shared Worker Pool Architecture

**Version:** 2.0  
**Date:** November 25, 2025  
**Critical Update:** L4/L5 as shared resources with WorkerPoolManager  
**Framework:** LangChain DeepAgents + LangGraph

---

## ðŸŽ¯ Overview

This guide shows how to implement the corrected Gold Standard System Prompts with the shared worker pool architecture.

### Key Architecture Changes from V1

**V1 (Incorrect):**
- Workers spawned as child agents
- Each expert had its own workers
- High token overhead per query

**V2 (Correct):**
- Workers in shared pool
- All experts use same pool
- Minimal token overhead

### Updated System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   System Architecture V2                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                           â”‚
â”‚  Postgres (Control Plane)                                â”‚
â”‚    â”œâ”€ agents table (with system_prompt_template_id)      â”‚
â”‚    â”œâ”€ system_prompt_templates table (NEW)                â”‚
â”‚    â”œâ”€ worker_pool_config table (NEW)                     â”‚
â”‚    â””â”€ worker_execution_log table (NEW)                   â”‚
â”‚                          â†“                                â”‚
â”‚  Prompt Rendering Layer                                   â”‚
â”‚    â”œâ”€ SystemPromptRenderer service                        â”‚
â”‚    â””â”€ Renders L1/L2/L3 prompts dynamically                â”‚
â”‚                          â†“                                â”‚
â”‚  Worker Pool Layer (NEW)                                  â”‚
â”‚    â”œâ”€ WorkerPoolManager service                           â”‚
â”‚    â”œâ”€ Manages 10-15 shared workers per type              â”‚
â”‚    â””â”€ Handles tenant isolation & scheduling               â”‚
â”‚                          â†“                                â”‚
â”‚  LangGraph Workflow Layer                                 â”‚
â”‚    â”œâ”€ Mode 1/2/3 workflows                                â”‚
â”‚    â”œâ”€ Integrates prompt rendering                         â”‚
â”‚    â””â”€ Provides worker pool access to agents               â”‚
â”‚                          â†“                                â”‚
â”‚  LangChain DeepAgents Runtime                             â”‚
â”‚    â”œâ”€ Creates agents with rendered prompts                â”‚
â”‚    â”œâ”€ Provides execute_worker_task() tool                 â”‚
â”‚    â””â”€ Executes with GraphRAG integration                  â”‚
â”‚                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“¦ Step 1: Database Schema Extensions

### Migration 1: System Prompt Templates

```sql
-- Migration: 20251125_add_system_prompt_templates.sql

CREATE TABLE system_prompt_templates (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    agent_level TEXT NOT NULL CHECK (agent_level IN ('MASTER', 'EXPERT', 'SPECIALIST', 'WORKER', 'TOOL')),
    template_name TEXT NOT NULL,
    base_template TEXT NOT NULL,  -- Base system prompt
    level_template TEXT NOT NULL,  -- Level-specific additions
    version TEXT NOT NULL DEFAULT '2.0',
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    metadata JSONB DEFAULT '{}'::jsonb,
    
    UNIQUE(agent_level, template_name, version)
);

CREATE INDEX idx_system_prompt_templates_level ON system_prompt_templates(agent_level);
CREATE INDEX idx_system_prompt_templates_active ON system_prompt_templates(is_active);

-- Add template reference to agents table
ALTER TABLE agents ADD COLUMN IF NOT EXISTS system_prompt_template_id UUID 
    REFERENCES system_prompt_templates(id);

-- Add prompt override capability
ALTER TABLE agents ADD COLUMN IF NOT EXISTS system_prompt_override TEXT;
ALTER TABLE agents ADD COLUMN IF NOT EXISTS prompt_variables JSONB DEFAULT '{}'::jsonb;

COMMENT ON COLUMN agents.system_prompt_template_id IS 'Reference to system prompt template';
COMMENT ON COLUMN agents.system_prompt_override IS 'Complete prompt override for testing';
COMMENT ON COLUMN agents.prompt_variables IS 'Agent-specific variables for template rendering';
```

### Migration 2: Worker Pool Infrastructure

```sql
-- Migration: 20251125_add_worker_pool_infrastructure.sql

-- Worker pool configuration
CREATE TABLE worker_pool_config (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    worker_type TEXT NOT NULL CHECK (worker_type IN ('data_extraction', 'computation', 'file_processing')),
    pool_size INTEGER NOT NULL DEFAULT 5,
    min_workers INTEGER NOT NULL DEFAULT 2,
    max_workers INTEGER NOT NULL DEFAULT 10,
    max_queue_size INTEGER NOT NULL DEFAULT 100,
    timeout_seconds INTEGER NOT NULL DEFAULT 30,
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    
    UNIQUE(worker_type)
);

-- Insert default pool configurations
INSERT INTO worker_pool_config (worker_type, pool_size, min_workers, max_workers) VALUES
('data_extraction', 5, 2, 10),
('computation', 5, 2, 10),
('file_processing', 3, 1, 5);

-- Worker execution log (for audit and billing)
CREATE TABLE worker_execution_log (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    worker_type TEXT NOT NULL,
    task TEXT NOT NULL,
    requesting_agent_id UUID NOT NULL REFERENCES agents(id),
    session_id UUID NOT NULL,
    tenant_id UUID,  -- For multi-tenant tracking
    execution_time_ms INTEGER NOT NULL,
    status TEXT NOT NULL CHECK (status IN ('COMPLETE', 'FAILED', 'PARTIAL')),
    input_params JSONB,
    output_data JSONB,  -- Sanitized output (no PII)
    error_message TEXT,
    timestamp TIMESTAMPTZ DEFAULT NOW(),
    
    -- Indexes for common queries
    INDEX idx_worker_log_tenant (tenant_id, timestamp),
    INDEX idx_worker_log_session (session_id),
    INDEX idx_worker_log_agent (requesting_agent_id),
    INDEX idx_worker_log_status (status),
    INDEX idx_worker_log_timestamp (timestamp)
);

-- Worker pool metrics (aggregated stats)
CREATE TABLE worker_pool_metrics (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    worker_type TEXT NOT NULL,
    tenant_id UUID,
    hour_bucket TIMESTAMPTZ NOT NULL,  -- Hourly aggregation
    total_executions INTEGER NOT NULL DEFAULT 0,
    successful_executions INTEGER NOT NULL DEFAULT 0,
    failed_executions INTEGER NOT NULL DEFAULT 0,
    avg_execution_time_ms FLOAT NOT NULL DEFAULT 0,
    total_execution_time_ms BIGINT NOT NULL DEFAULT 0,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    
    UNIQUE(worker_type, tenant_id, hour_bucket)
);

CREATE INDEX idx_worker_metrics_tenant ON worker_pool_metrics(tenant_id, hour_bucket);
CREATE INDEX idx_worker_metrics_type ON worker_pool_metrics(worker_type, hour_bucket);

-- Tool registry (for L5)
CREATE TABLE tool_registry (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    tool_name TEXT NOT NULL UNIQUE,
    tool_type TEXT NOT NULL CHECK (tool_type IN ('api_wrapper', 'calculator', 'data_processor')),
    function_name TEXT NOT NULL,
    input_schema JSONB NOT NULL,
    output_schema JSONB NOT NULL,
    rate_limit TEXT,  -- e.g., "100 calls/minute"
    timeout_seconds INTEGER NOT NULL DEFAULT 10,
    retries INTEGER NOT NULL DEFAULT 3,
    cost_per_call DECIMAL(10, 6) DEFAULT 0,
    is_active BOOLEAN DEFAULT true,
    version TEXT NOT NULL DEFAULT '1.0.0',
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    metadata JSONB DEFAULT '{}'::jsonb
);

CREATE INDEX idx_tool_registry_active ON tool_registry(is_active);
CREATE INDEX idx_tool_registry_type ON tool_registry(tool_type);

-- Tool execution log
CREATE TABLE tool_execution_log (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    tool_name TEXT NOT NULL,
    calling_agent_id UUID REFERENCES agents(id),
    calling_worker_id TEXT,  -- If called from worker
    session_id UUID,
    tenant_id UUID,
    execution_time_ms INTEGER NOT NULL,
    status TEXT NOT NULL CHECK (status IN ('SUCCESS', 'ERROR')),
    input_params JSONB,
    error_code TEXT,
    timestamp TIMESTAMPTZ DEFAULT NOW(),
    
    INDEX idx_tool_log_tenant (tenant_id, timestamp),
    INDEX idx_tool_log_tool (tool_name, timestamp)
);
```

### Migration 3: Seed Tool Registry

```sql
-- Migration: 20251125_seed_tool_registry.sql

-- Data Extraction Tools
INSERT INTO tool_registry (tool_name, tool_type, function_name, input_schema, output_schema, rate_limit) VALUES

('pubmed_api', 'api_wrapper', 'search_pubmed', 
 '{"type":"object","required":["query"],"properties":{"query":{"type":"string"},"max_results":{"type":"integer","default":100},"date_from":{"type":"string","format":"date"}}}'::jsonb,
 '{"type":"object","properties":{"articles":{"type":"array","items":{"type":"object","properties":{"pmid":{"type":"string"},"title":{"type":"string"},"abstract":{"type":"string"},"year":{"type":"integer"}}}},"count":{"type":"integer"}}}'::jsonb,
 '100 calls/minute'),

('fda_faers_api', 'api_wrapper', 'query_adverse_events',
 '{"type":"object","required":["drug_name"],"properties":{"drug_name":{"type":"string"},"date_from":{"type":"string"},"date_to":{"type":"string"},"event_types":{"type":"array","items":{"type":"string"}}}}'::jsonb,
 '{"type":"object","properties":{"events":{"type":"array","items":{"type":"object","properties":{"event_name":{"type":"string"},"frequency":{"type":"integer"},"serious":{"type":"boolean"}}}},"total_reports":{"type":"integer"}}}'::jsonb,
 '50 calls/minute'),

('clinicaltrials_gov_api', 'api_wrapper', 'search_trials',
 '{"type":"object","properties":{"condition":{"type":"string"},"intervention":{"type":"string"},"status":{"type":"string"},"max_results":{"type":"integer","default":100}}}'::jsonb,
 '{"type":"object","properties":{"trials":{"type":"array"},"count":{"type":"integer"}}}'::jsonb,
 '100 calls/minute');

-- Computation Tools
INSERT INTO tool_registry (tool_name, tool_type, function_name, input_schema, output_schema) VALUES

('sample_size_calculator', 'calculator', 'calculate_sample_size',
 '{"type":"object","required":["effect_size","alpha","power"],"properties":{"effect_size":{"type":"number"},"alpha":{"type":"number"},"power":{"type":"number"},"test":{"type":"string"},"dropout_rate":{"type":"number","default":0}}}'::jsonb,
 '{"type":"object","properties":{"n_per_group":{"type":"integer"},"n_adjusted":{"type":"integer"},"total_n":{"type":"integer"},"formula_used":{"type":"string"},"assumptions":{"type":"array","items":{"type":"string"}}}}'::jsonb),

('icer_calculator', 'calculator', 'calculate_icer',
 '{"type":"object","required":["incremental_cost","incremental_qaly"],"properties":{"incremental_cost":{"type":"number"},"incremental_qaly":{"type":"number"},"run_sensitivity":{"type":"boolean","default":false},"iterations":{"type":"integer","default":1000}}}'::jsonb,
 '{"type":"object","properties":{"icer":{"type":"number"},"ci_lower":{"type":"number"},"ci_upper":{"type":"number"},"cost_effective_prob":{"type":"number"}}}'::jsonb),

('statistical_calculator', 'calculator', 'run_statistical_test',
 '{"type":"object","required":["test_type","data"],"properties":{"test_type":{"type":"string","enum":["ttest","anova","chi_square"]},"data":{"type":"object"},"alpha":{"type":"number","default":0.05}}}'::jsonb,
 '{"type":"object","properties":{"test_statistic":{"type":"number"},"p_value":{"type":"number"},"confidence_interval":{"type":"array"},"interpretation":{"type":"string"}}}'::jsonb);

-- Data Processing Tools
INSERT INTO tool_registry (tool_name, tool_type, function_name, input_schema, output_schema) VALUES

('pdf_reader', 'data_processor', 'extract_from_pdf',
 '{"type":"object","required":["file_path"],"properties":{"file_path":{"type":"string"},"pages":{"type":"array","items":{"type":"integer"}},"extract_tables":{"type":"boolean","default":false},"keywords":{"type":"array","items":{"type":"string"}}}}'::jsonb,
 '{"type":"object","properties":{"text":{"type":"array","items":{"type":"string"}},"tables":{"type":"array"},"page_count":{"type":"integer"}}}'::jsonb),

('excel_parser', 'data_processor', 'parse_excel',
 '{"type":"object","required":["file_path"],"properties":{"file_path":{"type":"string"},"sheet_name":{"type":"string"},"header_row":{"type":"integer","default":0}}}'::jsonb,
 '{"type":"object","properties":{"data":{"type":"array"},"columns":{"type":"array"},"row_count":{"type":"integer"}}}'::jsonb),

('json_parser', 'data_processor', 'parse_json',
 '{"type":"object","required":["file_path"],"properties":{"file_path":{"type":"string"},"json_path":{"type":"string"}}}'::jsonb,
 '{"type":"object","properties":{"data":{"type":"object"},"keys":{"type":"array"}}}'::jsonb);
```

---

## ðŸ—ï¸ Step 2: Worker Pool Manager Service

### Complete Implementation (`services/worker_pool_manager.py`)

```python
"""
Worker Pool Manager Service
Manages shared pools of WORKER agents for L1/L2/L3 to use
"""

import asyncio
import logging
import time
from collections import defaultdict
from typing import Dict, Any, Optional, List
from uuid import UUID
from datetime import datetime, timedelta

from deepagents import create_agent
from database.connection import Database

logger = logging.getLogger(__name__)


class WorkerPoolManager:
    """
    Manages shared pools of WORKER agents.
    All L1/L2/L3 agents request workers from this pool.
    
    Features:
    - Tenant isolation
    - Fair scheduling
    - Usage tracking
    - Auto-scaling
    - Health monitoring
    """
    
    def __init__(
        self,
        db: Database,
        prompt_renderer: 'SystemPromptRenderer',  # Forward reference
    ):
        self.db = db
        self.prompt_renderer = prompt_renderer
        
        # Worker pools by type
        self.pools: Dict[str, WorkerPool] = {}
        
        # Metrics
        self.usage_metrics = {
            "total_executions": 0,
            "by_type": defaultdict(int),
            "by_tenant": defaultdict(int),
            "by_agent": defaultdict(int),
            "avg_execution_time": defaultdict(float),
            "failure_count": defaultdict(int),
        }
        
        # Queue for pending tasks (when all workers busy)
        self.task_queue: asyncio.Queue = asyncio.Queue(maxsize=1000)
        
        # Health check
        self.last_health_check = None
    
    async def initialize(self):
        """Initialize worker pools from database configuration."""
        configs = await self._load_pool_configs()
        
        for config in configs:
            worker_type = config['worker_type']
            pool_size = config['pool_size']
            
            logger.info(f"Initializing {worker_type} pool with {pool_size} workers")
            
            self.pools[worker_type] = WorkerPool(
                worker_type=worker_type,
                pool_size=pool_size,
                min_workers=config['min_workers'],
                max_workers=config['max_workers'],
                timeout_seconds=config['timeout_seconds'],
                prompt_renderer=self.prompt_renderer,
            )
            
            await self.pools[worker_type].initialize()
        
        logger.info(f"Worker pool manager initialized with {len(self.pools)} pools")
    
    async def execute_task(
        self,
        worker_type: str,
        task: str,
        params: dict,
        context: dict
    ) -> dict:
        """
        Execute a task using a worker from the shared pool.
        
        Args:
            worker_type: Type of worker (data_extraction, computation, file_processing)
            task: Specific task name
            params: Task parameters
            context: Execution context (requesting_agent, session_id, tenant_id)
            
        Returns:
            Task execution result
        """
        start_time = time.time()
        
        # Validate worker type
        pool = self.pools.get(worker_type)
        if not pool:
            return {
                "status": "FAILED",
                "error": {
                    "code": "INVALID_WORKER_TYPE",
                    "message": f"Unknown worker type: {worker_type}",
                    "valid_types": list(self.pools.keys())
                }
            }
        
        try:
            # Get available worker (blocks if all busy)
            worker = await pool.get_worker(timeout=60)
            
            if not worker:
                return {
                    "status": "FAILED",
                    "error": {
                        "code": "POOL_EXHAUSTED",
                        "message": f"All {worker_type} workers busy, timeout after 60s",
                        "recommendation": "Retry or reduce concurrent requests"
                    }
                }
            
            try:
                # Execute task
                result = await worker.execute(
                    task=task,
                    params=params,
                    context=context
                )
                
                execution_time = time.time() - start_time
                
                # Update metrics
                await self._update_metrics(
                    worker_type=worker_type,
                    tenant_id=context.get('tenant_id'),
                    agent_id=context.get('requesting_agent_id'),
                    execution_time=execution_time,
                    status=result['status']
                )
                
                # Log execution
                await self._log_execution(
                    worker_type=worker_type,
                    task=task,
                    context=context,
                    result=result,
                    execution_time=execution_time
                )
                
                return result
                
            finally:
                # Always return worker to pool
                await pool.release_worker(worker)
        
        except asyncio.TimeoutError:
            execution_time = time.time() - start_time
            return {
                "status": "FAILED",
                "error": {
                    "code": "TIMEOUT",
                    "message": f"Worker execution timeout after {execution_time:.1f}s"
                }
            }
        
        except Exception as e:
            logger.error(f"Worker execution error: {str(e)}", exc_info=True)
            execution_time = time.time() - start_time
            
            return {
                "status": "FAILED",
                "error": {
                    "code": "EXECUTION_ERROR",
                    "message": str(e)
                }
            }
    
    async def get_pool_status(self) -> dict:
        """Get current status of all worker pools."""
        return {
            pool_type: {
                "total_workers": pool.total_workers,
                "available": await pool.available_count(),
                "busy": await pool.busy_count(),
                "queued_tasks": pool.queue_size(),
                "health": pool.health_status()
            }
            for pool_type, pool in self.pools.items()
        }
    
    async def get_usage_metrics(
        self,
        tenant_id: Optional[UUID] = None,
        time_range: Optional[tuple] = None
    ) -> dict:
        """
        Get usage metrics, optionally filtered by tenant and time range.
        
        Args:
            tenant_id: Filter by tenant
            time_range: (start, end) datetime tuple
        """
        if tenant_id:
            # Get tenant-specific metrics
            if time_range:
                metrics = await self._get_tenant_metrics_range(tenant_id, time_range)
            else:
                metrics = {
                    "tenant_id": str(tenant_id),
                    "total_executions": self.usage_metrics["by_tenant"][tenant_id],
                    "avg_execution_time": self.usage_metrics["avg_execution_time"][tenant_id],
                    "failure_count": self.usage_metrics["failure_count"][tenant_id],
                }
            
            return metrics
        
        # Return overall metrics
        return {
            "total_executions": self.usage_metrics["total_executions"],
            "by_type": dict(self.usage_metrics["by_type"]),
            "pool_status": await self.get_pool_status(),
        }
    
    async def health_check(self) -> dict:
        """Perform health check on all pools."""
        self.last_health_check = datetime.now()
        
        health = {
            "overall_status": "HEALTHY",
            "timestamp": self.last_health_check.isoformat(),
            "pools": {}
        }
        
        for pool_type, pool in self.pools.items():
            pool_health = pool.health_status()
            health["pools"][pool_type] = pool_health
            
            if pool_health["status"] != "HEALTHY":
                health["overall_status"] = "DEGRADED"
        
        return health
    
    async def scale_pool(
        self,
        worker_type: str,
        target_size: int
    ):
        """Dynamically scale a worker pool."""
        pool = self.pools.get(worker_type)
        if not pool:
            raise ValueError(f"Unknown worker type: {worker_type}")
        
        await pool.scale_to(target_size)
        
        # Update config in database
        await self.db.execute(
            """
            UPDATE worker_pool_config
            SET pool_size = $1, updated_at = NOW()
            WHERE worker_type = $2
            """,
            target_size,
            worker_type
        )
    
    async def _load_pool_configs(self) -> List[dict]:
        """Load pool configurations from database."""
        query = """
            SELECT worker_type, pool_size, min_workers, max_workers, 
                   timeout_seconds
            FROM worker_pool_config
            WHERE is_active = true
            ORDER BY worker_type
        """
        
        results = await self.db.fetch(query)
        return [dict(r) for r in results]
    
    async def _update_metrics(
        self,
        worker_type: str,
        tenant_id: Optional[UUID],
        agent_id: Optional[UUID],
        execution_time: float,
        status: str
    ):
        """Update usage metrics."""
        self.usage_metrics["total_executions"] += 1
        self.usage_metrics["by_type"][worker_type] += 1
        
        if tenant_id:
            self.usage_metrics["by_tenant"][tenant_id] += 1
            
            # Update rolling average
            current_avg = self.usage_metrics["avg_execution_time"][tenant_id]
            count = self.usage_metrics["by_tenant"][tenant_id]
            new_avg = ((current_avg * (count - 1)) + execution_time) / count
            self.usage_metrics["avg_execution_time"][tenant_id] = new_avg
        
        if agent_id:
            self.usage_metrics["by_agent"][agent_id] += 1
        
        if status == "FAILED":
            self.usage_metrics["failure_count"][worker_type] += 1
    
    async def _log_execution(
        self,
        worker_type: str,
        task: str,
        context: dict,
        result: dict,
        execution_time: float
    ):
        """Log worker execution for audit trail."""
        await self.db.execute(
            """
            INSERT INTO worker_execution_log
            (worker_type, task, requesting_agent_id, session_id, tenant_id,
             execution_time_ms, status, input_params, output_data, error_message)
            VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
            """,
            worker_type,
            task,
            context.get('requesting_agent_id'),
            context.get('session_id'),
            context.get('tenant_id'),
            int(execution_time * 1000),
            result['status'],
            None,  # input_params - sanitize if needed
            None,  # output_data - sanitize if needed
            result.get('error', {}).get('message')
        )
        
        # Aggregate metrics hourly
        await self._aggregate_metrics_hourly(
            worker_type=worker_type,
            tenant_id=context.get('tenant_id'),
            execution_time=execution_time,
            status=result['status']
        )
    
    async def _aggregate_metrics_hourly(
        self,
        worker_type: str,
        tenant_id: Optional[UUID],
        execution_time: float,
        status: str
    ):
        """Aggregate metrics into hourly buckets."""
        hour_bucket = datetime.now().replace(minute=0, second=0, microsecond=0)
        
        await self.db.execute(
            """
            INSERT INTO worker_pool_metrics
            (worker_type, tenant_id, hour_bucket, total_executions,
             successful_executions, failed_executions, avg_execution_time_ms,
             total_execution_time_ms)
            VALUES ($1, $2, $3, 1, $4, $5, $6, $7)
            ON CONFLICT (worker_type, tenant_id, hour_bucket)
            DO UPDATE SET
                total_executions = worker_pool_metrics.total_executions + 1,
                successful_executions = worker_pool_metrics.successful_executions + EXCLUDED.successful_executions,
                failed_executions = worker_pool_metrics.failed_executions + EXCLUDED.failed_executions,
                total_execution_time_ms = worker_pool_metrics.total_execution_time_ms + EXCLUDED.total_execution_time_ms,
                avg_execution_time_ms = 
                    (worker_pool_metrics.total_execution_time_ms + EXCLUDED.total_execution_time_ms)::float /
                    (worker_pool_metrics.total_executions + 1)
            """,
            worker_type,
            tenant_id,
            hour_bucket,
            1 if status == "COMPLETE" else 0,
            1 if status == "FAILED" else 0,
            execution_time * 1000,
            int(execution_time * 1000)
        )
    
    async def _get_tenant_metrics_range(
        self,
        tenant_id: UUID,
        time_range: tuple
    ) -> dict:
        """Get tenant metrics for specific time range."""
        start, end = time_range
        
        query = """
            SELECT 
                SUM(total_executions) as total_executions,
                SUM(successful_executions) as successful,
                SUM(failed_executions) as failed,
                AVG(avg_execution_time_ms) as avg_time_ms
            FROM worker_pool_metrics
            WHERE tenant_id = $1
              AND hour_bucket >= $2
              AND hour_bucket < $3
        """
        
        result = await self.db.fetch_one(query, tenant_id, start, end)
        
        return {
            "tenant_id": str(tenant_id),
            "time_range": {
                "start": start.isoformat(),
                "end": end.isoformat()
            },
            "total_executions": result['total_executions'] or 0,
            "successful_executions": result['successful'] or 0,
            "failed_executions": result['failed'] or 0,
            "avg_execution_time_ms": float(result['avg_time_ms'] or 0),
        }


class WorkerPool:
    """Pool of identical workers for a specific type."""
    
    def __init__(
        self,
        worker_type: str,
        pool_size: int,
        min_workers: int,
        max_workers: int,
        timeout_seconds: int,
        prompt_renderer: 'SystemPromptRenderer',
    ):
        self.worker_type = worker_type
        self.pool_size = pool_size
        self.min_workers = min_workers
        self.max_workers = max_workers
        self.timeout_seconds = timeout_seconds
        self.prompt_renderer = prompt_renderer
        
        # Worker management
        self.workers: List['SharedWorker'] = []
        self.available_workers: asyncio.Queue = asyncio.Queue()
        self.busy_workers: set = set()
        
        # Health tracking
        self.total_executions = 0
        self.failed_executions = 0
        self.last_failure_time = None
    
    async def initialize(self):
        """Initialize the worker pool."""
        for i in range(self.pool_size):
            worker = await self._create_worker(i)
            self.workers.append(worker)
            await self.available_workers.put(worker)
        
        logger.info(f"Initialized {self.worker_type} pool with {self.pool_size} workers")
    
    async def _create_worker(self, worker_id: int) -> 'SharedWorker':
        """Create a worker instance."""
        # Load generic worker prompt for this type
        worker_prompt = await self._load_worker_prompt()
        
        return SharedWorker(
            worker_id=f"{self.worker_type}_{worker_id}",
            worker_type=self.worker_type,
            system_prompt=worker_prompt,
            timeout_seconds=self.timeout_seconds,
        )
    
    async def _load_worker_prompt(self) -> str:
        """Load generic worker prompt for this type."""
        # This loads the L4 system prompt template
        # It's generic and reused across all workers of this type
        template_query = """
            SELECT base_template, level_template
            FROM system_prompt_templates
            WHERE agent_level = 'WORKER'
              AND is_active = true
            ORDER BY created_at DESC
            LIMIT 1
        """
        
        result = await self.prompt_renderer.agent_repo.db.fetch_one(template_query)
        
        if not result:
            raise ValueError("No WORKER system prompt template found")
        
        # Combine base + level template
        prompt = result['base_template'] + "\n\n" + result['level_template']
        
        # Substitute worker-type-specific variables
        prompt = prompt.replace('{{worker_type}}', self.worker_type)
        
        return prompt
    
    async def get_worker(self, timeout: int = 60) -> Optional['SharedWorker']:
        """
        Get an available worker from the pool.
        Blocks if all workers are busy (up to timeout).
        """
        try:
            worker = await asyncio.wait_for(
                self.available_workers.get(),
                timeout=timeout
            )
            self.busy_workers.add(worker.worker_id)
            return worker
        except asyncio.TimeoutError:
            logger.warning(f"Timeout waiting for {self.worker_type} worker")
            return None
    
    async def release_worker(self, worker: 'SharedWorker'):
        """Return worker to pool."""
        # Reset worker state
        worker.reset()
        
        # Remove from busy set
        self.busy_workers.discard(worker.worker_id)
        
        # Return to available queue
        await self.available_workers.put(worker)
    
    async def available_count(self) -> int:
        """Number of available workers."""
        return self.available_workers.qsize()
    
    async def busy_count(self) -> int:
        """Number of busy workers."""
        return len(self.busy_workers)
    
    def queue_size(self) -> int:
        """Number of tasks waiting for workers."""
        # Would need additional tracking
        return 0
    
    def health_status(self) -> dict:
        """Get health status of this pool."""
        if self.total_executions == 0:
            failure_rate = 0
        else:
            failure_rate = self.failed_executions / self.total_executions
        
        # Determine health status
        if failure_rate > 0.5:
            status = "UNHEALTHY"
        elif failure_rate > 0.2:
            status = "DEGRADED"
        else:
            status = "HEALTHY"
        
        return {
            "status": status,
            "total_workers": len(self.workers),
            "available_workers": self.available_workers.qsize(),
            "busy_workers": len(self.busy_workers),
            "total_executions": self.total_executions,
            "failed_executions": self.failed_executions,
            "failure_rate": failure_rate,
            "last_failure": self.last_failure_time.isoformat() if self.last_failure_time else None
        }
    
    async def scale_to(self, target_size: int):
        """Scale pool to target size."""
        if target_size < self.min_workers:
            target_size = self.min_workers
        elif target_size > self.max_workers:
            target_size = self.max_workers
        
        current_size = len(self.workers)
        
        if target_size > current_size:
            # Add workers
            for i in range(current_size, target_size):
                worker = await self._create_worker(i)
                self.workers.append(worker)
                await self.available_workers.put(worker)
            
            logger.info(f"Scaled {self.worker_type} pool up to {target_size}")
        
        elif target_size < current_size:
            # Remove workers (only available ones)
            removed = 0
            while removed < (current_size - target_size):
                try:
                    worker = self.available_workers.get_nowait()
                    self.workers.remove(worker)
                    removed += 1
                except asyncio.QueueEmpty:
                    break
            
            logger.info(f"Scaled {self.worker_type} pool down to {len(self.workers)}")


class SharedWorker:
    """
    A stateless worker instance.
    Can be reused for multiple tasks without state contamination.
    """
    
    def __init__(
        self,
        worker_id: str,
        worker_type: str,
        system_prompt: str,
        timeout_seconds: int,
    ):
        self.worker_id = worker_id
        self.worker_type = worker_type
        self.system_prompt = system_prompt
        self.timeout_seconds = timeout_seconds
        self.current_task = None
        
        # Create the actual DeepAgent instance
        # This is created once and reused
        self.agent = create_agent(
            model="anthropic:claude-haiku-4-5-20251001",  # Fast, cheap model
            system_prompt=system_prompt,
            tools=self._get_tools_for_type(),
        )
    
    async def execute(
        self,
        task: str,
        params: dict,
        context: dict
    ) -> dict:
        """
        Execute a task with given parameters.
        Worker is stateless - all info must be in parameters.
        """
        self.current_task = task
        
        try:
            # Build task message with complete context
            task_message = self._build_task_message(task, params, context)
            
            # Execute with timeout
            result = await asyncio.wait_for(
                self.agent.invoke({
                    "messages": [{"role": "user", "content": task_message}]
                }),
                timeout=self.timeout_seconds
            )
            
            # Parse structured output
            return self._parse_result(result)
            
        except asyncio.TimeoutError:
            return {
                "status": "FAILED",
                "error": {
                    "code": "TIMEOUT",
                    "message": f"Worker execution timeout after {self.timeout_seconds}s"
                }
            }
        
        except Exception as e:
            logger.error(f"Worker {self.worker_id} execution error: {str(e)}")
            return {
                "status": "FAILED",
                "error": {
                    "code": "EXECUTION_ERROR",
                    "message": str(e)
                }
            }
        
        finally:
            self.current_task = None
    
    def reset(self):
        """Reset worker state between tasks."""
        self.current_task = None
        # Important: Agent itself is stateless, no reset needed
    
    def _build_task_message(self, task: str, params: dict, context: dict) -> str:
        """Build task message with all necessary context."""
        import json
        
        return f"""
Execute task: {task}

Parameters:
{json.dumps(params, indent=2)}

Context:
- Requesting Agent: {context.get('requesting_agent_id', 'unknown')}
- Session: {context.get('session_id', 'unknown')}
- Tenant: {context.get('tenant_id', 'unknown')}

Return structured JSON output according to task schema.
"""
    
    def _get_tools_for_type(self) -> list:
        """Get tools available for this worker type."""
        tool_mapping = {
            "data_extraction": [
                "pubmed_api",
                "fda_faers_api",
                "clinicaltrials_gov_api",
                "web_scraper"
            ],
            "computation": [
                "sample_size_calculator",
                "icer_calculator",
                "statistical_calculator"
            ],
            "file_processing": [
                "pdf_reader",
                "excel_parser",
                "json_parser"
            ],
        }
        return tool_mapping.get(self.worker_type, [])
    
    def _parse_result(self, agent_result: dict) -> dict:
        """Parse agent result into standard format."""
        import json
        
        content = agent_result['messages'][-1]['content']
        
        # Parse JSON from content
        try:
            parsed = json.loads(content)
            return {
                "status": parsed.get("status", "COMPLETE"),
                "task_id": self.worker_id,
                "execution_time_ms": agent_result.get('usage', {}).get('latency_ms', 0),
                "output": parsed.get("output", {}),
                "metadata": parsed.get("metadata", {}),
            }
        except json.JSONDecodeError:
            # Agent didn't return valid JSON
            return {
                "status": "ERROR",
                "error": {
                    "code": "INVALID_OUTPUT",
                    "message": "Worker returned non-JSON output"
                },
                "raw_output": content[:500]  # First 500 chars for debugging
            }
```

---

## ðŸ”§ Step 3: Integration with LangGraph Workflows

### Updated Mode 1 Workflow (`langgraph_workflows/mode1_interactive_manual.py`)

```python
from services.worker_pool_manager import WorkerPoolManager
from services.prompt_renderer import SystemPromptRenderer

class Mode1InteractiveManualWorkflow:
    def __init__(self, db, ...):
        # ... existing init ...
        
        # Add prompt renderer
        self.prompt_renderer = SystemPromptRenderer(
            agent_repo=self.agent_repo,
            rag_repo=self.rag_repo,
            kg_view_resolver=self.kg_view_resolver
        )
        
        # Add worker pool manager
        self.worker_pool = WorkerPoolManager(
            db=db,
            prompt_renderer=self.prompt_renderer
        )
    
    async def initialize(self):
        """Initialize workflow components."""
        await self.worker_pool.initialize()
        logger.info("Mode 1 workflow initialized with worker pool")
    
    async def execute_expert_agent_node(self, state: UnifiedWorkflowState):
        """Execute EXPERT agent with worker pool access."""
        
        # 1. Render expert system prompt dynamically
        system_prompt = await self.prompt_renderer.render_system_prompt(
            agent_id=state['selected_agent_id'],
            context={
                'tier': state['tier'],
                'session_id': state['session_id'],
                'query': state['query'],
                'tenant_id': state.get('tenant_id'),
            }
        )
        
        logger.info(f"Rendered system prompt: {len(system_prompt)} chars")
        
        # 2. Create DeepAgent with worker pool tool
        expert_agent = create_agent(
            model="anthropic:claude-sonnet-4-5-20250929",
            system_prompt=system_prompt,
            tools=[
                "write_todos",
                "write_file",
                "read_file",
                "task",  # For spawning specialists
                
                # Add worker pool tool
                {
                    "name": "execute_worker_task",
                    "description": """
                    Execute a task using a shared worker from the pool.
                    
                    This is how you access data extraction, computation, 
                    and file processing capabilities.
                    
                    Examples:
                    - execute_worker_task(worker_type="data_extraction", task="pubmed_search", params={...})
                    - execute_worker_task(worker_type="computation", task="sample_size_calculation", params={...})
                    - execute_worker_task(worker_type="file_processing", task="pdf_extraction", params={...})
                    """,
                    "input_schema": {
                        "type": "object",
                        "required": ["worker_type", "task", "params"],
                        "properties": {
                            "worker_type": {
                                "type": "string",
                                "enum": ["data_extraction", "computation", "file_processing"],
                                "description": "Type of worker to use"
                            },
                            "task": {
                                "type": "string",
                                "description": "Specific task name (e.g., 'pubmed_search', 'sample_size_calculation')"
                            },
                            "params": {
                                "type": "object",
                                "description": "Task-specific parameters"
                            }
                        }
                    },
                    "function": lambda worker_type, task, params: self._execute_worker_task_wrapper(
                        worker_type=worker_type,
                        task=task,
                        params=params,
                        context={
                            'requesting_agent_id': state['selected_agent_id'],
                            'session_id': state['session_id'],
                            'tenant_id': state.get('tenant_id'),
                        }
                    )
                }
            ],
            checkpointer=self.postgres_checkpointer,
            thread_id=str(state['session_id']),
        )
        
        # 3. Execute expert query
        result = await expert_agent.invoke({
            "messages": [{"role": "user", "content": state['query']}]
        })
        
        # 4. Update state with result
        state['agent_response'] = result['messages'][-1]['content']
        state['agent_execution_metadata'] = {
            'tokens_used': result.get('usage', {}).get('total_tokens', 0),
            'execution_time': result.get('execution_time', 0),
            'tools_used': [tool['name'] for tool in result.get('tool_calls', [])],
            'worker_tasks_executed': len([t for t in result.get('tool_calls', []) if t['name'] == 'execute_worker_task']),
        }
        
        return state
    
    async def _execute_worker_task_wrapper(
        self,
        worker_type: str,
        task: str,
        params: dict,
        context: dict
    ) -> dict:
        """Wrapper to call worker pool from tool."""
        return await self.worker_pool.execute_task(
            worker_type=worker_type,
            task=task,
            params=params,
            context=context
        )
```

---

## ðŸ§ª Step 4: Testing & Validation

### Test Suite (`tests/test_worker_pool.py`)

```python
import pytest
import asyncio
from uuid import UUID

from services.worker_pool_manager import WorkerPoolManager

@pytest.mark.asyncio
async def test_worker_pool_initialization():
    """Test worker pool initializes correctly."""
    
    worker_pool = WorkerPoolManager(db=test_db, prompt_renderer=mock_renderer)
    await worker_pool.initialize()
    
    # Verify pools created
    assert 'data_extraction' in worker_pool.pools
    assert 'computation' in worker_pool.pools
    assert 'file_processing' in worker_pool.pools
    
    # Verify pool sizes
    status = await worker_pool.get_pool_status()
    assert status['data_extraction']['total_workers'] == 5
    assert status['computation']['total_workers'] == 5
    assert status['file_processing']['total_workers'] == 3


@pytest.mark.asyncio
async def test_worker_execution():
    """Test worker executes task successfully."""
    
    worker_pool = WorkerPoolManager(db=test_db, prompt_renderer=mock_renderer)
    await worker_pool.initialize()
    
    result = await worker_pool.execute_task(
        worker_type="computation",
        task="sample_size_calculation",
        params={
            "effect_size": 0.8,
            "alpha": 0.05,
            "power": 0.90,
            "test": "two_sample_ttest"
        },
        context={
            'requesting_agent_id': UUID('...'),
            'session_id': UUID('...'),
            'tenant_id': UUID('...')
        }
    )
    
    assert result['status'] == 'COMPLETE'
    assert 'n_per_group' in result['output']
    assert result['output']['n_per_group'] > 0


@pytest.mark.asyncio
async def test_worker_pool_concurrency():
    """Test worker pool handles concurrent requests."""
    
    worker_pool = WorkerPoolManager(db=test_db, prompt_renderer=mock_renderer)
    await worker_pool.initialize()
    
    # Launch 10 concurrent requests (pool has 5 workers)
    tasks = [
        worker_pool.execute_task(
            worker_type="computation",
            task="sample_size_calculation",
            params={"effect_size": 0.8, "alpha": 0.05, "power": 0.90},
            context={'requesting_agent_id': UUID('...'), 'session_id': UUID('...'), 'tenant_id': UUID('...')}
        )
        for _ in range(10)
    ]
    
    results = await asyncio.gather(*tasks)
    
    # All should complete successfully
    assert all(r['status'] == 'COMPLETE' for r in results)


@pytest.mark.asyncio
async def test_worker_tenant_isolation():
    """Test workers maintain tenant isolation."""
    
    worker_pool = WorkerPoolManager(db=test_db, prompt_renderer=mock_renderer)
    await worker_pool.initialize()
    
    tenant1_id = UUID('...')
    tenant2_id = UUID('...')
    
    # Execute tasks for different tenants
    result1 = await worker_pool.execute_task(
        worker_type="data_extraction",
        task="test_task",
        params={},
        context={'tenant_id': tenant1_id, ...}
    )
    
    result2 = await worker_pool.execute_task(
        worker_type="data_extraction",
        task="test_task",
        params={},
        context={'tenant_id': tenant2_id, ...}
    )
    
    # Verify metrics tracked separately
    metrics1 = await worker_pool.get_usage_metrics(tenant_id=tenant1_id)
    metrics2 = await worker_pool.get_usage_metrics(tenant_id=tenant2_id)
    
    assert metrics1['total_executions'] >= 1
    assert metrics2['total_executions'] >= 1
```

---

## ðŸ“Š Step 5: Monitoring & Optimization

### Grafana Dashboard Configuration

```yaml
# grafana-dashboards/worker-pool-monitoring.json

{
  "dashboard": {
    "title": "Worker Pool Monitoring",
    "panels": [
      {
        "title": "Pool Status",
        "type": "stat",
        "targets": [
          {
            "query": "SELECT worker_type, total_workers, available_workers FROM worker_pool_status"
          }
        ]
      },
      {
        "title": "Execution Rate",
        "type": "graph",
        "targets": [
          {
            "query": "SELECT hour_bucket, SUM(total_executions) FROM worker_pool_metrics GROUP BY hour_bucket"
          }
        ]
      },
      {
        "title": "Avg Execution Time",
        "type": "graph",
        "targets": [
          {
            "query": "SELECT hour_bucket, worker_type, AVG(avg_execution_time_ms) FROM worker_pool_metrics GROUP BY hour_bucket, worker_type"
          }
        ]
      },
      {
        "title": "Failure Rate",
        "type": "graph",
        "targets": [
          {
            "query": "SELECT hour_bucket, (SUM(failed_executions)::float / SUM(total_executions)) * 100 FROM worker_pool_metrics GROUP BY hour_bucket"
          }
        ]
      },
      {
        "title": "Usage by Tenant",
        "type": "table",
        "targets": [
          {
            "query": "SELECT tenant_id, SUM(total_executions), AVG(avg_execution_time_ms) FROM worker_pool_metrics WHERE hour_bucket > NOW() - INTERVAL '24 hours' GROUP BY tenant_id ORDER BY SUM(total_executions) DESC"
          }
        ]
      }
    ]
  }
}
```

---

## ðŸš€ Step 6: Deployment

### Deployment Checklist

**Week 1: Database & Services**
- [ ] Run migrations (20251125_add_system_prompt_templates.sql)
- [ ] Run migrations (20251125_add_worker_pool_infrastructure.sql)
- [ ] Run migrations (20251125_seed_tool_registry.sql)
- [ ] Seed L4/L5 system prompt templates
- [ ] Deploy WorkerPoolManager service
- [ ] Deploy updated PromptRenderer service

**Week 2: Integration**
- [ ] Update Mode 1 workflow with worker pool integration
- [ ] Update Mode 2 workflow (if applicable)
- [ ] Test L1 â†’ Worker Pool
- [ ] Test L2 â†’ Worker Pool
- [ ] Test L3 â†’ Worker Pool

**Week 3: Testing**
- [ ] Unit tests for WorkerPoolManager
- [ ] Integration tests for workflows
- [ ] Load testing (100 concurrent requests)
- [ ] Multi-tenant isolation testing
- [ ] Performance benchmarking

**Week 4: Production**
- [ ] Deploy to staging environment
- [ ] 10% traffic test (A/B with legacy)
- [ ] Monitor metrics for 48 hours
- [ ] 50% traffic rollout
- [ ] 100% traffic (full migration)
- [ ] Deprecate legacy worker spawning code

---

## ðŸ“ˆ Expected Performance Improvements

### Token Efficiency

```
Before (Hierarchical):
- Expert: 1800 tokens
- Worker spawn: 800 tokens (full prompt)
- Tool spawn: 500 tokens (full prompt)
Average query: 3100 tokens

After (Shared Pool):
- Expert: 1800 tokens
- Worker call: 200 tokens (task data only)
- Tool call: 100 tokens (params only)
Average query: 2100 tokens

Savings: 32% token reduction
```

### Latency Improvements

```
Before:
- Worker spawn time: 2-3s
- Tool spawn time: 1-2s
- Total overhead: 3-5s per query

After:
- Worker available immediately: 0s
- Tool available immediately: 0s
- Total overhead: 0s

Latency improvement: 3-5s per query
```

### Cost Savings (1000 queries/day)

```
Token cost (@$0.015/1K tokens):
- Before: 3.1M tokens/day = $46.50/day = $1,395/month
- After: 2.1M tokens/day = $31.50/day = $945/month
Monthly savings: $450

Compute cost (reduced latency):
- Faster responses = lower compute time
- Estimated additional savings: $200/month

Total monthly savings: ~$650
Annual savings: ~$7,800
```

---

## ðŸŽ¯ Key Takeaways

1. **Shared Resources are More Efficient**: 10-15 workers serving 319+ experts
2. **Token Optimization**: 32% reduction per query
3. **Latency Improvement**: 3-5s faster per query
4. **Multi-Tenancy Ready**: Built-in tenant isolation and tracking
5. **Scalable**: Easy to add more workers as load increases
6. **Maintainable**: Update L4/L5 prompts once â†’ affects all usage

---

**Questions? Need help with implementation? Let me know!**
