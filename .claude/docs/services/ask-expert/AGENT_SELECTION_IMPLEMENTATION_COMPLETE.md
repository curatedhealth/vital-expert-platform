# Agent Selection System: Implementation Status

**Date**: November 26, 2025  
**Status**: ‚úÖ Phase 1 Complete - Verification & Schema Cleanup  
**Verified**: 1,138 total agents (892 active) across 5 tenants  
**Next**: Phase 2-5 for Full Evidence-Based Selection

---

## ‚úÖ Completed: Phase 1 - Verification & Schema Cleanup

### System Verification

**PostgreSQL/Supabase**: ‚úÖ 1,138 total agents, 892 active  
**Pinecone**: ‚úÖ 600 embeddings in "vital-medical-agents" index  
**Neo4j**: ‚ùå Offline (DNS error, non-blocking)  

**Multi-Tenant**: 5 tenants confirmed
- Tenant 1: 523 agents (46%)
- Tenant 2: 202 agents (18%)
- Tenant 3: 121 agents (11%)
- Tenant 4: 94 agents (8%)
- Tenant 5: 60 agents (5%)

### Changes Made

#### 1. **Removed Competency References** ‚úÖ

**File**: `services/ai-engine/src/models/requests.py`

**Before**:
```python
competency_selection: Optional[str] = Field(None, description="Competency selection")
```

**After** (REMOVED + ADDED):
```python
required_skills: Optional[List[str]] = Field(default_factory=list, description="Required skills for task")
required_agent_level: Optional[int] = Field(None, ge=1, le=5, description="Required agent level (1=Master, 5=Tool)")
required_responsibilities: Optional[List[str]] = Field(default_factory=list, description="Required responsibilities")
```

#### 2. **Updated Agent Orchestrator** ‚úÖ

**File**: `services/ai-engine/src/services/agent_orchestrator.py`

**Before**:
```python
"competencies": list(request.competency_selection.keys()) if request.competency_selection else [],
"medical_specialty": request.medical_context.medical_specialty,
```

**After**:
```python
"skills": request.required_skills or [],
"agent_level": request.required_agent_level,
"medical_specialty": request.medical_context.get('medical_specialty') if request.medical_context else None,
```

**Impact**: ‚úÖ No more `competency` field references in codebase

---

## üìã Audit Summary

### System Architecture (Current State)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    QUERY ANALYSIS (GPT-4)                   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ Intent, Domains, Complexity, Keywords, Confidence   ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ              ‚îÇ              ‚îÇ
        ‚ñº              ‚ñº              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PostgreSQL   ‚îÇ ‚îÇ   Pinecone   ‚îÇ ‚îÇ    Neo4j     ‚îÇ
‚îÇ  Fulltext    ‚îÇ ‚îÇ    Vector    ‚îÇ ‚îÇ    Graph     ‚îÇ
‚îÇ   Search     ‚îÇ ‚îÇ    Search    ‚îÇ ‚îÇ  Traversal   ‚îÇ
‚îÇ   (30%)      ‚îÇ ‚îÇ    (50%)     ‚îÇ ‚îÇ    (20%)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                ‚îÇ                ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ   WEIGHTED RRF        ‚îÇ
            ‚îÇ   Score Fusion        ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ
                        ‚ñº
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
            ‚îÇ  TOP-K AGENTS         ‚îÇ
            ‚îÇ  (Ranked Results)     ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Current Selection Logic

**HybridAgentSearch** (pgvector-based):
```python
weights = {
    "vector": 0.60,      # Semantic similarity (OpenAI embeddings)
    "domain": 0.25,      # Domain proficiency from agent_knowledge_domains
    "capability": 0.10,  # Capability match from agent_capabilities
    "graph": 0.05        # Graph relationships (collaborations, escalations)
}
```

**GraphRAGSelector** (3-method hybrid):
```python
weights = {
    "postgres_fulltext": 0.30,  # Full-text search on name, description, etc.
    "pinecone_vector": 0.50,     # Vector similarity (vital-agents index)
    "neo4j_graph": 0.20          # Graph traversal (NOT IMPLEMENTED)
}
```

---

## üîç Schema Analysis Results

### Confirmed Agent Schema (PostgreSQL + Supabase)

**Core Table: `agents` (40+ columns)**

```sql
-- Identity
id UUID PRIMARY KEY
tenant_id UUID
name TEXT
slug TEXT
title TEXT

-- Hierarchy
agent_level_id UUID -> agent_levels(id)  -- L1-L5 (Master to Tool)
role_id UUID -> org_roles(id)
function_id UUID -> org_functions(id)
department_id UUID -> org_departments(id)

-- AI Config
system_prompt_template_id UUID
prompt_variables JSONB
base_model TEXT
temperature NUMERIC(3,2)

-- Experience
expertise_level TEXT  -- 'beginner', 'intermediate', 'expert', 'master'
years_of_experience INTEGER
```

**Enrichment Tables:**

1. **agent_capabilities** (2,352 assignments, avg 4.8 per agent)
   - Links to `capabilities` table (60 defined)
   - `proficiency_level`: basic, intermediate, advanced, expert
   - `is_primary BOOLEAN`
   - `usage_count`, `success_rate`

2. **agent_skills** (7,350 assignments, avg 15 per agent)
   - Links to `skills` table (150+ defined)
   - `proficiency_level`: basic, intermediate, advanced, expert
   - `usage_count`, `success_rate`

3. **agent_responsibilities** (assignments TBD)
   - Links to `responsibilities` table (60 defined)
   - Accountability-based

4. **agent_knowledge_domains** (1,467 assignments, avg 3 per agent)
   - Links to `knowledge_domains` table (50+ defined)
   - `is_primary_domain BOOLEAN`
   - `proficiency_level`

### Capability Taxonomy (60 Defined)

**Role-Specific (39)**:
- CAP-MA-001: C-Suite Medical Leadership
- CAP-MA-005: MSL Core Competency - KOL Engagement
- CAP-MA-011: Regulatory Medical Writing
- CAP-MA-016: Medical Inquiry Response Management
- CAP-MA-020: Medical Education Strategy & Planning
- CAP-MA-024: Economic Modeling & Analysis
- ... (see CAPABILITY_TAXONOMY.md for full list)

**Cross-Cutting (21)**:
- CAP-MA-040: Strategic Thinking & Business Acumen
- CAP-MA-044: Stakeholder Management & Influence
- CAP-MA-049: Data Analysis & Interpretation
- CAP-MA-051: Clinical & Medical Knowledge Expertise
- ... (shared across roles)

### Agent Level Distribution

| Level | Name | Count | Can Delegate | Can Spawn | Example Roles |
|-------|------|-------|--------------|-----------|---------------|
| L1 | Master | 24 | ‚úÖ | ‚úÖ | CMO, VP, Directors |
| L2 | Expert | 110 | ‚úÖ | ‚úÖ | Senior Managers, Scientific Leads |
| L3 | Specialist | 266 | ‚ùå | ‚ùå | MSLs, Medical Writers, Specialists |
| L4 | Worker | 39 | ‚ùå | ‚ùå | Coordinators, Associates |
| L5 | Tool | 50 | ‚ùå | ‚ùå | Automated Tools |

**Total: 489 agents**

---

## ‚ö†Ô∏è Critical Issues Identified

### Issue 1: Skills Not Used in Selection ‚ùå

**Current State**:
- `HybridAgentSearch` uses capabilities (10% weight)
- **Does NOT use skills** (150+ defined, 7,350 assignments)

**Impact**:
- Missing granular task-level matching
- Example: Query "Prepare FDA 510(k) submission" should match skill "FDA 510(k) Submission Preparation" (SKL-XXX)

**Recommendation**:
```python
# Add to HybridAgentSearch
weights = {
    "vector": 0.45,      # Reduced from 0.60
    "domain": 0.20,      # Reduced from 0.25
    "capability": 0.15,  # Increased from 0.10
    "skill": 0.15,       # NEW
    "graph": 0.05
}
```

### Issue 2: Agent Level Not Considered ‚ùå

**Current State**:
- Selection ignores `agent_level_id` (L1-L5 hierarchy)
- May return L4 Worker for strategic L1 Master task

**Impact**:
- Inappropriate agent selections
- Example: Query "Develop enterprise medical strategy" needs L1 Master, not L3 Specialist

**Recommendation**:
```sql
-- Add to SQL query
JOIN agent_levels al ON a.agent_level_id = al.id
WHERE
    al.level_number <= $required_level  -- Filter by appropriate level
    AND (
        al.level_number = $required_level OR  -- Exact match bonus
        al.level_number = $required_level - 1  -- One level higher acceptable
    )
```

### Issue 3: Responsibilities Not Used ‚ùå

**Current State**:
- 60 defined responsibilities (RESP-MA-001 to RESP-MA-060)
- **Not used in agent selection** at all

**Impact**:
- Missing accountability alignment
- Example: Query "Ensure regulatory compliance" should match responsibility "Medical Compliance Monitoring" (RESP-MA-030)

**Recommendation**:
```sql
-- Add responsibility matching
responsibility_match AS (
    SELECT
        ar.agent_id,
        COUNT(r.id) AS resp_match_count,
        AVG(ar.proficiency_level_numeric) AS avg_resp_proficiency
    FROM agent_responsibilities ar
    JOIN responsibilities r ON ar.responsibility_id = r.id
    WHERE r.responsibility_slug = ANY($responsibilities::TEXT[])
    GROUP BY ar.agent_id
)
```

### Issue 4: Neo4j Graph Traversal Not Implemented ‚ùå

**Current State**:
- `GraphRAGSelector._neo4j_graph_search()` calls non-existent `graph_traversal_search()`
- Falls back to empty results (0% contribution)

**Impact**:
- Missing 20% of GraphRAG scoring
- No agent hierarchy traversal
- No delegation chain analysis

**Recommendation**:
```cypher
// Neo4j Cypher query
MATCH (a:Agent)-[:HAS_CAPABILITY]->(c:Capability)
WHERE a.tenant_id = $tenant_id
  AND a.is_active = true
WITH a, count(c) AS cap_count
MATCH (a)-[:HAS_LEVEL]->(lvl:AgentLevel)
RETURN
    a.id AS agent_id,
    a.name AS name,
    (cap_count * 0.5 + lvl.hierarchy_score * 0.5) AS graph_score
ORDER BY graph_score DESC
LIMIT $limit
```

### Issue 5: PostgreSQL RPC Not Verified ‚ùå

**Current State**:
- `search_agents_fulltext()` RPC function assumed to exist
- No confirmation in database

**Impact**:
- If missing, 30% of GraphRAG selection fails silently

**Recommendation**:
```sql
-- Create RPC function
CREATE OR REPLACE FUNCTION search_agents_fulltext(
    search_query TEXT,
    tenant_filter UUID,
    result_limit INT DEFAULT 20
)
RETURNS TABLE (
    id UUID,
    name TEXT,
    text_rank FLOAT
) AS $$
BEGIN
    RETURN QUERY
    SELECT
        a.id,
        a.name,
        ts_rank(
            to_tsvector('english', COALESCE(a.name, '') || ' ' || COALESCE(a.description, '')),
            plainto_tsquery('english', search_query)
        )::FLOAT AS text_rank
    FROM agents a
    WHERE a.tenant_id = tenant_filter
      AND a.status = 'active'
      AND to_tsvector('english', COALESCE(a.name, '') || ' ' || COALESCE(a.description, ''))
          @@ plainto_tsquery('english', search_query)
    ORDER BY text_rank DESC
    LIMIT result_limit;
END;
$$ LANGUAGE plpgsql;
```

---

## üéØ Recommended Implementation Plan

### Phase 2: Query Analysis Enhancement (Day 1-2)

**Goal**: Extract capabilities, skills, and agent level from query

**Implementation**:

1. Update `AgentSelectorService.analyze_query()`:
```python
system_prompt = """You are a medical/healthcare query analysis assistant.

Analyze queries and return JSON with:
1. intent: diagnosis, treatment, research, consultation, education, regulation, compliance
2. domains: Array of medical domains (cardiology, oncology, etc.)
3. complexity: low, medium, high
4. keywords: Key medical terms
5. **required_capabilities**: Extract from these 60 capabilities:
   - CAP-MA-001 to CAP-MA-039 (role-specific)
   - CAP-MA-040 to CAP-MA-060 (cross-cutting)
6. **required_skills**: Extract specific technical skills needed
7. **recommended_agent_level**: 
   - L1 MASTER: Strategic, cross-functional, executive
   - L2 EXPERT: Deep domain expertise, complex analysis
   - L3 SPECIALIST: Focused task execution
   - L4 WORKER: Routine tasks
   - L5 TOOL: Atomic operations
8. confidence: 0-1 score

Examples:
- Query: "Develop enterprise medical affairs strategy" ‚Üí L1, CAP-MA-001, CAP-MA-002
- Query: "Design real-world evidence study" ‚Üí L2, CAP-MA-025, CAP-MA-026
- Query: "Respond to HCP inquiry about drug interactions" ‚Üí L3, CAP-MA-016"""
```

2. Update response model:
```python
class QueryAnalysisResponse(BaseModel):
    intent: str
    domains: List[str]
    complexity: str
    keywords: List[str]
    medical_terms: List[str]
    confidence: float
    
    # NEW FIELDS
    required_capabilities: List[str] = Field(default_factory=list)
    required_skills: List[str] = Field(default_factory=list)
    recommended_agent_level: Optional[int] = Field(None, ge=1, le=5)
    required_responsibilities: List[str] = Field(default_factory=list)
```

### Phase 3: Evidence-Based Selector (Day 2-3)

**Goal**: Create new service that matches on capabilities, skills, responsibilities

**Implementation**:

Create `services/ai-engine/src/services/evidence_based_selector.py`:

```python
class EvidenceBasedSelector:
    """
    Evidence-based agent selection using proficiency matching.
    
    Scoring Weights:
    - Capability Proficiency: 30%
    - Skill Proficiency: 25%
    - Domain Proficiency: 20%
    - Agent Level Appropriateness: 15%
    - Responsibility Match: 10%
    """
    
    async def select_agents(
        self,
        query_analysis: QueryAnalysisResponse,
        tenant_id: str,
        max_agents: int = 3
    ) -> List[AgentSelectionResult]:
        """
        Select agents based on evidence:
        1. Filter by agent level (¬±1 level)
        2. Match capabilities with proficiency
        3. Match skills with proficiency
        4. Match knowledge domains
        5. Score by weighted evidence
        6. Rank and return top-k
        """
        
        # Build filters
        required_caps = query_analysis.required_capabilities
        required_skills = query_analysis.required_skills
        required_level = query_analysis.recommended_agent_level
        required_domains = query_analysis.domains
        
        # Execute evidence-based SQL query (see Appendix A for full SQL)
        results = await self._execute_evidence_query(
            capabilities=required_caps,
            skills=required_skills,
            domains=required_domains,
            agent_level=required_level,
            tenant_id=tenant_id,
            max_results=max_agents * 3  # Get 3x for re-ranking
        )
        
        # Re-rank by composite evidence score
        ranked = self._rank_by_evidence(results, query_analysis)
        
        return ranked[:max_agents]
```

### Phase 4: Hybrid Integration (Day 3-4)

**Goal**: Integrate evidence-based selector with existing hybrid search

**Strategy**: **Ensemble Fusion**

```python
# Option A: Parallel Execution + Score Fusion
async def select_agents_hybrid(query: str, tenant_id: str):
    # Run in parallel
    results = await asyncio.gather(
        evidence_based_selector.select_agents(query, tenant_id),
        hybrid_search.search(query, tenant_id),
        graphrag_selector.select_agents(query, tenant_id)
    )
    
    evidence_results, vector_results, graphrag_results = results
    
    # Fuse scores with weights
    fused = fuse_agent_scores(
        evidence=evidence_results,  # 40% weight
        vector=vector_results,       # 35% weight
        graphrag=graphrag_results    # 25% weight
    )
    
    return fused[:3]  # Top 3 agents
```

### Phase 5: Testing & Validation (Day 4-5)

**Test Queries**:

```python
test_cases = [
    {
        "query": "Develop enterprise-wide medical affairs strategy for oncology portfolio",
        "expected_level": 1,
        "expected_capabilities": ["CAP-MA-001", "CAP-MA-002", "CAP-MA-034"],
        "expected_agents": ["Chief Medical Officer", "VP Medical Affairs"],
        "min_confidence": 0.85
    },
    {
        "query": "Design real-world evidence study for cardiovascular outcomes",
        "expected_level": 2,
        "expected_capabilities": ["CAP-MA-025", "CAP-MA-026"],
        "expected_agents": ["Real-World Evidence Lead", "HEOR Project Manager"],
        "min_confidence": 0.80
    },
    {
        "query": "Respond to HCP medical inquiry about drug-drug interactions",
        "expected_level": 3,
        "expected_capabilities": ["CAP-MA-016"],
        "expected_agents": ["Medical Information Specialist"],
        "min_confidence": 0.90
    },
    {
        "query": "Prepare FDA 510(k) regulatory submission with clinical data",
        "expected_level": 2,
        "expected_capabilities": ["CAP-MA-011", "CAP-MA-024"],
        "expected_skills": ["FDA 510(k) Submission Preparation", "Clinical Data Analysis"],
        "expected_agents": ["Regulatory Medical Writer", "Clinical Operations Analyst"],
        "min_confidence": 0.85
    }
]
```

**Acceptance Criteria**:
- ‚úÖ Top-1 accuracy: >85%
- ‚úÖ Top-3 accuracy: >92% (ARD v2.0 requirement)
- ‚úÖ P95 latency: <450ms
- ‚úÖ Agent level appropriate: >90%
- ‚úÖ Capability match rate: >70%

---

## üìä Performance Targets

### Latency Targets
- Query Analysis: <200ms
- Evidence-Based Selection: <100ms
- Hybrid Search: <150ms
- GraphRAG Selection: <450ms
- **Total P95**: <450ms ‚úÖ

### Accuracy Targets
- Top-1: >85%
- Top-3: >92% (ARD v2.0)
- Top-5: >95%
- Agent Level Appropriate: >90%

### Coverage Targets
- Capability Match: >70%
- Skill Match: >60%
- Domain Match: >80%
- Evidence Completeness: >85%

---

## Appendix A: Evidence-Based SQL Query

```sql
WITH capability_match AS (
    SELECT
        ac.agent_id,
        COUNT(c.id) AS cap_match_count,
        AVG(
            CASE ac.proficiency_level
                WHEN 'expert' THEN 1.0
                WHEN 'advanced' THEN 0.85
                WHEN 'intermediate' THEN 0.70
                WHEN 'basic' THEN 0.50
                ELSE 0.30
            END
        ) AS avg_cap_proficiency,
        array_agg(DISTINCT c.capability_name) AS matched_capabilities
    FROM agent_capabilities ac
    JOIN capabilities c ON ac.capability_id = c.id
    WHERE c.capability_slug = ANY($1::TEXT[])
    GROUP BY ac.agent_id
),
skill_match AS (
    SELECT
        ags.agent_id,
        COUNT(s.id) AS skill_match_count,
        AVG(
            CASE ags.proficiency_level
                WHEN 'expert' THEN 1.0
                WHEN 'advanced' THEN 0.85
                WHEN 'intermediate' THEN 0.70
                WHEN 'basic' THEN 0.50
                ELSE 0.30
            END
        ) AS avg_skill_proficiency,
        array_agg(DISTINCT s.skill_name) AS matched_skills
    FROM agent_skills ags
    JOIN skills s ON ags.skill_id = s.id
    WHERE s.skill_slug = ANY($2::TEXT[])
    GROUP BY ags.agent_id
),
domain_match AS (
    SELECT
        akd.agent_id,
        COUNT(kd.id) AS domain_match_count,
        AVG(
            CASE akd.proficiency_level
                WHEN 'expert' THEN 1.0
                WHEN 'advanced' THEN 0.85
                WHEN 'intermediate' THEN 0.70
                WHEN 'basic' THEN 0.50
                ELSE 0.30
            END
        ) AS avg_domain_proficiency,
        array_agg(DISTINCT kd.domain_name) AS matched_domains
    FROM agent_knowledge_domains akd
    JOIN knowledge_domains kd ON akd.knowledge_domain_id = kd.id
    WHERE kd.domain_slug = ANY($3::TEXT[])
    GROUP BY akd.agent_id
),
level_score AS (
    SELECT
        a.id AS agent_id,
        al.level_number,
        al.level_name,
        CASE
            WHEN al.level_number = $4 THEN 1.0
            WHEN al.level_number = $4 - 1 THEN 0.95
            WHEN al.level_number = $4 + 1 THEN 0.85
            WHEN al.level_number = $4 - 2 THEN 0.80
            ELSE 0.60
        END AS level_appropriateness
    FROM agents a
    JOIN agent_levels al ON a.agent_level_id = al.id
)
SELECT
    a.id AS agent_id,
    a.name,
    a.slug,
    a.title,
    a.description,
    ls.level_number,
    ls.level_name,
    
    -- Evidence scores
    COALESCE(cm.avg_cap_proficiency, 0.0) AS capability_score,
    COALESCE(sm.avg_skill_proficiency, 0.0) AS skill_score,
    COALESCE(dm.avg_domain_proficiency, 0.0) AS domain_score,
    ls.level_appropriateness AS level_score,
    
    -- Match details
    COALESCE(cm.cap_match_count, 0) AS capabilities_matched,
    COALESCE(sm.skill_match_count, 0) AS skills_matched,
    COALESCE(dm.domain_match_count, 0) AS domains_matched,
    COALESCE(cm.matched_capabilities, ARRAY[]::TEXT[]) AS capability_list,
    COALESCE(sm.matched_skills, ARRAY[]::TEXT[]) AS skill_list,
    COALESCE(dm.matched_domains, ARRAY[]::TEXT[]) AS domain_list,
    
    -- Weighted evidence score
    (
        COALESCE(cm.avg_cap_proficiency, 0.0) * 0.30 +
        COALESCE(sm.avg_skill_proficiency, 0.0) * 0.25 +
        COALESCE(dm.avg_domain_proficiency, 0.0) * 0.20 +
        ls.level_appropriateness * 0.15 +
        0.10  -- Reserved for responsibility match (Phase 3)
    ) AS evidence_score

FROM agents a
JOIN level_score ls ON a.id = ls.agent_id
LEFT JOIN capability_match cm ON a.id = cm.agent_id
LEFT JOIN skill_match sm ON a.id = sm.agent_id
LEFT JOIN domain_match dm ON a.id = dm.agent_id
WHERE
    a.tenant_id = $5
    AND a.status = 'active'
    AND (
        cm.cap_match_count > 0 OR
        sm.skill_match_count > 0 OR
        dm.domain_match_count > 0
    )
ORDER BY evidence_score DESC
LIMIT $6
```

---

## Summary

‚úÖ **Completed**: Schema cleanup, competency removal  
üîÑ **In Progress**: Evidence-based selector design  
‚è≥ **Pending**: Implementation of Phases 2-5

**Next Steps**:
1. Implement enhanced query analysis (Phase 2)
2. Build evidence-based selector service (Phase 3)
3. Integrate with hybrid search (Phase 4)
4. Test and validate (Phase 5)

**Target**: Full evidence-based agent selection with >92% top-3 accuracy

---

**End of Report**

