"""
Integration Tests for All 4 Workflow Modes

Tests end-to-end execution with REAL LLMs (OpenAI GPT-4).

âš ï¸ IMPORTANT: These tests make real API calls and incur costs.
Run selectively with: pytest -k "test_mode1" or pytest -k "integration"

Test Coverage:
- Mode 1: Interactive + Automatic (auto agent selection, multi-turn)
- Mode 2: Interactive + Manual (user-selected agent, multi-turn)
- Mode 3: Autonomous + Automatic (auto agent selection, one-shot)
- Mode 4: Autonomous + Manual (user-selected agent, one-shot)

All tests:
- Use real OpenAI GPT-4 API
- Execute full workflows end-to-end
- Verify responses, state, and outputs
- Test with actual RAG/tools if available
- Measure performance and costs

Environment Variables Required:
- OPENAI_API_KEY
- SUPABASE_URL
- SUPABASE_KEY
- REDIS_URL (optional, falls back to memory cache)
"""

import pytest
import asyncio
import os
from datetime import datetime, timezone
from uuid import uuid4
from typing import Dict, Any

# Skip all tests if no API key (CI/CD)
pytestmark = pytest.mark.skipif(
    not os.getenv("OPENAI_API_KEY"),
    reason="Integration tests require OPENAI_API_KEY"
)


# ============================================================================
# FIXTURES & HELPERS
# ============================================================================

@pytest.fixture(scope="module")
def integration_config():
    """Integration test configuration."""
    return {
        "openai_api_key": os.getenv("OPENAI_API_KEY"),
        "supabase_url": os.getenv("SUPABASE_URL", "http://localhost:54321"),
        "supabase_key": os.getenv("SUPABASE_KEY", ""),
        "redis_url": os.getenv("REDIS_URL"),
        "test_tenant_id": os.getenv("TEST_TENANT_ID", str(uuid4())),
        "test_user_id": os.getenv("TEST_USER_ID", str(uuid4())),
        "use_real_llm": True,
        "model": "gpt-4",
        "max_cost_per_test": 0.50,  # $0.50 max per test
    }


@pytest.fixture
async def supabase_client(integration_config):
    """Real Supabase client for integration tests."""
    try:
        from supabase import create_client, Client
        
        if not integration_config["supabase_key"]:
            pytest.skip("SUPABASE_KEY not configured")
        
        client = create_client(
            integration_config["supabase_url"],
            integration_config["supabase_key"]
        )
        
        yield client
    except ImportError:
        pytest.skip("supabase-py not installed")
    except Exception as e:
        pytest.skip(f"Supabase client creation failed: {e}")


@pytest.fixture
async def cache_manager(integration_config):
    """Cache manager (Redis or memory fallback)."""
    from services.cache_manager import CacheManager
    
    cache = CacheManager(redis_url=integration_config.get("redis_url"))
    await cache.initialize()
    
    yield cache
    
    # Cleanup
    await cache.close()


@pytest.fixture
async def rag_service(supabase_client):
    """Real RAG service."""
    from services.unified_rag_service import UnifiedRAGService
    
    rag = UnifiedRAGService(supabase_client)
    await rag.initialize()
    
    return rag


# ============================================================================
# MODE 1: INTERACTIVE + AUTOMATIC TESTS
# ============================================================================

@pytest.mark.integration
@pytest.mark.asyncio
async def test_mode1_basic_query_real_llm(integration_config, supabase_client, cache_manager, rag_service):
    """
    Test Mode 1 with real LLM: Automatic agent selection, simple query.
    
    Expected: Agent selected, response generated, state tracked.
    """
    from langgraph_workflows.mode1_interactive_auto_workflow import Mode1InteractiveAutoWorkflow
    from langgraph_workflows.state_schemas import WorkflowMode, ExecutionStatus
    
    workflow = Mode1InteractiveAutoWorkflow(
        supabase_client=supabase_client,
        cache_manager=cache_manager,
        rag_service=rag_service
    )
    
    await workflow.initialize()
    
    # Execute simple query
    result = await workflow.execute(
        tenant_id=integration_config["test_tenant_id"],
        user_id=integration_config["test_user_id"],
        session_id=f"mode1_test_{datetime.now().timestamp()}",
        query="What are the key requirements for FDA IND submission?",
        model=integration_config["model"],
        enable_rag=True,
        enable_tools=False,
        max_iterations=1
    )
    
    # Assertions
    assert result is not None
    assert result.get("status") == ExecutionStatus.COMPLETED
    assert "response" in result
    assert len(result["response"]) > 0
    assert "agent_id" in result or "selected_agents" in result
    
    print(f"\nâœ… Mode 1 Integration Test Passed")
    print(f"Response length: {len(result['response'])} chars")
    print(f"Cost: ${result.get('total_cost_usd', 0):.4f}")


@pytest.mark.integration
@pytest.mark.asyncio
async def test_mode1_multi_turn_conversation(integration_config, supabase_client, cache_manager):
    """
    Test Mode 1 multi-turn conversation with context.
    
    Expected: Context maintained, follow-up questions work.
    """
    from langgraph_workflows.mode1_interactive_auto_workflow import Mode1InteractiveAutoWorkflow
    
    workflow = Mode1InteractiveAutoWorkflow(
        supabase_client=supabase_client,
        cache_manager=cache_manager
    )
    
    await workflow.initialize()
    
    session_id = f"mode1_multi_{datetime.now().timestamp()}"
    
    # Turn 1
    result1 = await workflow.execute(
        tenant_id=integration_config["test_tenant_id"],
        user_id=integration_config["test_user_id"],
        session_id=session_id,
        query="What is clinical trial Phase 2?",
        model=integration_config["model"],
        max_iterations=1
    )
    
    assert result1["status"] == ExecutionStatus.COMPLETED
    
    # Turn 2 (follow-up)
    result2 = await workflow.execute(
        tenant_id=integration_config["test_tenant_id"],
        user_id=integration_config["test_user_id"],
        session_id=session_id,
        query="How long does it typically last?",
        model=integration_config["model"],
        conversation_history=result1.get("conversation_history", []),
        max_iterations=1
    )
    
    assert result2["status"] == ExecutionStatus.COMPLETED
    assert len(result2["response"]) > 0
    
    print(f"\nâœ… Mode 1 Multi-Turn Test Passed")
    print(f"Total cost: ${result1.get('total_cost_usd', 0) + result2.get('total_cost_usd', 0):.4f}")


# ============================================================================
# MODE 2: INTERACTIVE + MANUAL TESTS
# ============================================================================

@pytest.mark.integration
@pytest.mark.asyncio
async def test_mode2_manual_agent_selection(integration_config, supabase_client, cache_manager):
    """
    Test Mode 2 with user-selected agent.
    
    Expected: Uses specified agent, generates response.
    """
    from langgraph_workflows.mode2_interactive_manual_workflow import Mode2InteractiveManualWorkflow
    from langgraph_workflows.state_schemas import ExecutionStatus
    
    workflow = Mode2InteractiveManualWorkflow(
        supabase_client=supabase_client,
        cache_manager=cache_manager
    )
    
    await workflow.initialize()
    
    # Note: Requires a valid agent_id in the database
    # For testing, we'll use a mock agent_id
    test_agent_id = "test_agent_regulatory"
    
    result = await workflow.execute(
        tenant_id=integration_config["test_tenant_id"],
        user_id=integration_config["test_user_id"],
        session_id=f"mode2_test_{datetime.now().timestamp()}",
        query="Explain FDA 510(k) clearance process",
        selected_agents=[test_agent_id],
        model=integration_config["model"],
        enable_rag=True,
        max_iterations=1
    )
    
    # If agent not found, test should handle gracefully
    if result.get("status") == ExecutionStatus.FAILED:
        assert "agent" in result.get("error", "").lower() or "not found" in result.get("error", "").lower()
        print(f"\nâš ï¸  Mode 2 Test: Agent validation worked (agent not found as expected)")
    else:
        assert result["status"] == ExecutionStatus.COMPLETED
        assert len(result["response"]) > 0
        print(f"\nâœ… Mode 2 Integration Test Passed")


# ============================================================================
# MODE 3: AUTONOMOUS + AUTOMATIC TESTS
# ============================================================================

@pytest.mark.integration
@pytest.mark.asyncio
async def test_mode3_autonomous_reasoning(integration_config, supabase_client, cache_manager, rag_service):
    """
    Test Mode 3: Autonomous workflow with ReAct reasoning.
    
    Expected: Multi-step reasoning, tool usage, goal achievement.
    """
    from langgraph_workflows.mode3_autonomous_auto_workflow import Mode3AutonomousAutoWorkflow
    from langgraph_workflows.state_schemas import ExecutionStatus
    
    workflow = Mode3AutonomousAutoWorkflow(
        supabase_client=supabase_client,
        cache_manager=cache_manager,
        rag_service=rag_service
    )
    
    await workflow.initialize()
    
    result = await workflow.execute(
        tenant_id=integration_config["test_tenant_id"],
        user_id=integration_config["test_user_id"],
        session_id=f"mode3_test_{datetime.now().timestamp()}",
        query="Research FDA IND requirements and create a 3-point checklist",
        model=integration_config["model"],
        enable_rag=True,
        enable_tools=False,  # Disable web tools to reduce cost
        max_iterations=3,  # Allow multi-step reasoning
        cost_limit_usd=0.50
    )
    
    # Assertions
    assert result is not None
    assert result.get("status") in [ExecutionStatus.COMPLETED, ExecutionStatus.PARTIAL]
    assert "response" in result
    assert len(result["response"]) > 0
    
    # Check for autonomous reasoning artifacts
    assert "iteration_history" in result or "iterations_used" in result
    
    print(f"\nâœ… Mode 3 Integration Test Passed")
    print(f"Iterations used: {result.get('iterations_used', 'N/A')}")
    print(f"Goal achieved: {result.get('goal_achieved', 'N/A')}")
    print(f"Cost: ${result.get('total_cost_usd', 0):.4f}")


@pytest.mark.integration
@pytest.mark.asyncio
async def test_mode3_with_tool_chain(integration_config, supabase_client, cache_manager, rag_service):
    """
    Test Mode 3 with tool chaining (multi-step in one iteration).
    
    Expected: Tool chain executed, synthesis produced.
    """
    from langgraph_workflows.mode3_autonomous_auto_workflow import Mode3AutonomousAutoWorkflow
    from langgraph_workflows.state_schemas import ExecutionStatus
    
    workflow = Mode3AutonomousAutoWorkflow(
        supabase_client=supabase_client,
        cache_manager=cache_manager,
        rag_service=rag_service
    )
    
    await workflow.initialize()
    
    result = await workflow.execute(
        tenant_id=integration_config["test_tenant_id"],
        user_id=integration_config["test_user_id"],
        session_id=f"mode3_chain_{datetime.now().timestamp()}",
        query="Search regulations AND analyze key requirements",
        model=integration_config["model"],
        enable_rag=True,
        enable_tools=False,
        max_iterations=2,
        cost_limit_usd=0.50
    )
    
    assert result is not None
    assert result.get("status") in [ExecutionStatus.COMPLETED, ExecutionStatus.PARTIAL]
    
    # Check if tool chain was used
    if "tool_chain_used" in result:
        print(f"\nâœ… Mode 3 Tool Chain Test Passed")
        print(f"Tool chain used: {result['tool_chain_used']}")
    else:
        print(f"\nâœ… Mode 3 Test Passed (no tool chain triggered)")


# ============================================================================
# MODE 4: AUTONOMOUS + MANUAL TESTS
# ============================================================================

@pytest.mark.integration
@pytest.mark.asyncio
async def test_mode4_autonomous_manual_agent(integration_config, supabase_client, cache_manager, rag_service):
    """
    Test Mode 4: Autonomous reasoning with user-selected agent.
    
    Expected: Fixed agent used, autonomous reasoning, goal-based completion.
    """
    from langgraph_workflows.mode4_autonomous_manual_workflow import Mode4AutonomousManualWorkflow
    from langgraph_workflows.state_schemas import ExecutionStatus
    
    workflow = Mode4AutonomousManualWorkflow(
        supabase_client=supabase_client,
        cache_manager=cache_manager,
        rag_service=rag_service
    )
    
    await workflow.initialize()
    
    test_agent_id = "test_agent_regulatory"
    
    result = await workflow.execute(
        tenant_id=integration_config["test_tenant_id"],
        user_id=integration_config["test_user_id"],
        session_id=f"mode4_test_{datetime.now().timestamp()}",
        query="Analyze FDA compliance requirements for medical devices",
        selected_agents=[test_agent_id],
        model=integration_config["model"],
        enable_rag=True,
        max_iterations=3,
        cost_limit_usd=0.50
    )
    
    # Agent validation might fail in test environment
    if result.get("status") == ExecutionStatus.FAILED:
        assert "agent" in result.get("error", "").lower()
        print(f"\nâš ï¸  Mode 4 Test: Agent validation worked (expected in test env)")
    else:
        assert result["status"] in [ExecutionStatus.COMPLETED, ExecutionStatus.PARTIAL]
        print(f"\nâœ… Mode 4 Integration Test Passed")
        print(f"Fixed agent: {test_agent_id}")
        print(f"Iterations: {result.get('iterations_used', 'N/A')}")


# ============================================================================
# CROSS-MODE COMPARISON TESTS
# ============================================================================

@pytest.mark.integration
@pytest.mark.asyncio
async def test_all_modes_same_query_comparison(integration_config, supabase_client, cache_manager, rag_service):
    """
    Test all 4 modes with the same query and compare results.
    
    Expected: All modes produce valid responses, Mode 3/4 may be more comprehensive.
    """
    from langgraph_workflows.mode1_interactive_auto_workflow import Mode1InteractiveAutoWorkflow
    from langgraph_workflows.mode3_autonomous_auto_workflow import Mode3AutonomousAutoWorkflow
    
    query = "What is a clinical trial protocol?"
    session_prefix = f"comparison_{datetime.now().timestamp()}"
    
    results = {}
    
    # Mode 1
    try:
        wf1 = Mode1InteractiveAutoWorkflow(supabase_client, cache_manager, rag_service)
        await wf1.initialize()
        results["mode1"] = await wf1.execute(
            tenant_id=integration_config["test_tenant_id"],
            user_id=integration_config["test_user_id"],
            session_id=f"{session_prefix}_mode1",
            query=query,
            model=integration_config["model"],
            max_iterations=1
        )
    except Exception as e:
        results["mode1"] = {"error": str(e)}
    
    # Mode 3
    try:
        wf3 = Mode3AutonomousAutoWorkflow(supabase_client, cache_manager, rag_service)
        await wf3.initialize()
        results["mode3"] = await wf3.execute(
            tenant_id=integration_config["test_tenant_id"],
            user_id=integration_config["test_user_id"],
            session_id=f"{session_prefix}_mode3",
            query=query,
            model=integration_config["model"],
            max_iterations=2,
            cost_limit_usd=0.30
        )
    except Exception as e:
        results["mode3"] = {"error": str(e)}
    
    # Compare
    print(f"\nðŸ“Š Cross-Mode Comparison:")
    for mode, result in results.items():
        if "error" in result:
            print(f"  {mode}: âŒ Error - {result['error']}")
        else:
            status = result.get("status", "unknown")
            response_len = len(result.get("response", ""))
            cost = result.get("total_cost_usd", 0)
            print(f"  {mode}: âœ… {status} | {response_len} chars | ${cost:.4f}")
    
    # At least one mode should succeed
    successful_modes = [m for m, r in results.items() if "error" not in r]
    assert len(successful_modes) > 0, "At least one mode should complete successfully"


# ============================================================================
# PERFORMANCE & COST TESTS
# ============================================================================

@pytest.mark.integration
@pytest.mark.asyncio
async def test_mode3_cost_enforcement(integration_config, supabase_client, cache_manager, rag_service):
    """
    Test that Mode 3 respects cost limits.
    
    Expected: Stops when cost limit reached.
    """
    from langgraph_workflows.mode3_autonomous_auto_workflow import Mode3AutonomousAutoWorkflow
    
    workflow = Mode3AutonomousAutoWorkflow(supabase_client, cache_manager, rag_service)
    await workflow.initialize()
    
    result = await workflow.execute(
        tenant_id=integration_config["test_tenant_id"],
        user_id=integration_config["test_user_id"],
        session_id=f"mode3_cost_{datetime.now().timestamp()}",
        query="Comprehensive analysis of all FDA submission pathways",
        model=integration_config["model"],
        enable_rag=True,
        max_iterations=10,  # High limit
        cost_limit_usd=0.10  # Low cost limit (should stop early)
    )
    
    total_cost = result.get("total_cost_usd", 0)
    
    # Should stop at or near the limit
    assert total_cost <= 0.15, f"Cost ${total_cost:.4f} exceeded limit + buffer"
    
    print(f"\nâœ… Cost Enforcement Test Passed")
    print(f"Cost limit: $0.10")
    print(f"Actual cost: ${total_cost:.4f}")


@pytest.mark.integration
@pytest.mark.asyncio  
async def test_mode3_runtime_enforcement(integration_config, supabase_client, cache_manager, rag_service):
    """
    Test that Mode 3 respects runtime limits.
    
    Expected: Stops when time limit reached.
    """
    from langgraph_workflows.mode3_autonomous_auto_workflow import Mode3AutonomousAutoWorkflow
    
    workflow = Mode3AutonomousAutoWorkflow(supabase_client, cache_manager, rag_service)
    await workflow.initialize()
    
    start_time = datetime.now()
    
    result = await workflow.execute(
        tenant_id=integration_config["test_tenant_id"],
        user_id=integration_config["test_user_id"],
        session_id=f"mode3_time_{datetime.now().timestamp()}",
        query="Detailed analysis requiring many steps",
        model=integration_config["model"],
        max_iterations=20,  # High limit
        runtime_limit_minutes=0.5,  # 30 seconds (should stop early)
        cost_limit_usd=1.0
    )
    
    duration_seconds = (datetime.now() - start_time).total_seconds()
    
    # Should stop within ~30 seconds (+ buffer for overhead)
    assert duration_seconds < 60, f"Runtime {duration_seconds}s exceeded limit + buffer"
    
    print(f"\nâœ… Runtime Enforcement Test Passed")
    print(f"Time limit: 30s")
    print(f"Actual runtime: {duration_seconds:.1f}s")


# ============================================================================
# ERROR HANDLING & RECOVERY TESTS
# ============================================================================

@pytest.mark.integration
@pytest.mark.asyncio
async def test_mode3_handles_llm_errors_gracefully(integration_config, supabase_client, cache_manager):
    """
    Test Mode 3 handles LLM errors gracefully.
    
    Expected: Continues or fails gracefully with error details.
    """
    from langgraph_workflows.mode3_autonomous_auto_workflow import Mode3AutonomousAutoWorkflow
    from langgraph_workflows.state_schemas import ExecutionStatus
    
    workflow = Mode3AutonomousAutoWorkflow(supabase_client, cache_manager)
    await workflow.initialize()
    
    # Use invalid model to trigger error
    result = await workflow.execute(
        tenant_id=integration_config["test_tenant_id"],
        user_id=integration_config["test_user_id"],
        session_id=f"mode3_error_{datetime.now().timestamp()}",
        query="Test error handling",
        model="invalid-model-name",  # Should fail
        max_iterations=1
    )
    
    # Should fail gracefully
    assert result.get("status") == ExecutionStatus.FAILED
    assert "error" in result or "errors" in result
    
    print(f"\nâœ… Error Handling Test Passed")
    print(f"Error captured: {result.get('error', result.get('errors', 'N/A'))[:100]}")


if __name__ == "__main__":
    """Run integration tests with cost warnings."""
    print("=" * 80)
    print("âš ï¸  INTEGRATION TESTS - REAL API CALLS & COSTS")
    print("=" * 80)
    print("\nThese tests will:")
    print("  â€¢ Call OpenAI GPT-4 API (costs money)")
    print("  â€¢ Execute full workflows end-to-end")
    print("  â€¢ Take several minutes to complete")
    print(f"\nEstimated cost: $0.50 - $2.00 per full test run")
    print("\nEnvironment variables required:")
    print("  â€¢ OPENAI_API_KEY")
    print("  â€¢ SUPABASE_URL (optional)")
    print("  â€¢ SUPABASE_KEY (optional)")
    print("\nRun specific modes:")
    print("  pytest -k 'mode1' -v")
    print("  pytest -k 'mode3' -v")
    print("=" * 80)
    
    # Run with pytest
    pytest.main([__file__, "-v", "-s", "--tb=short"])

