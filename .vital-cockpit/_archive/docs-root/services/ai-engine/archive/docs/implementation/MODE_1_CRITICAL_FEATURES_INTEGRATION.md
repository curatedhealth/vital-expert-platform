# üöÄ MODE 1: CRITICAL FEATURES INTEGRATION

**Date:** November 1, 2025  
**Purpose:** Integrate GraphRAG, LangExtract, Knowledge Enrichment, and Agent Memory  
**Status:** ENHANCEMENT PLAN

---

## üéØ CRITICAL FEATURES TO INTEGRATE

### ‚úÖ **1. GraphRAG for Agents** (ALREADY IMPLEMENTED!)

**File:** `services/graph_relationship_builder.py`

**Features:**
- ‚úÖ Agent-Domain relationships with proficiency scores
- ‚úÖ Agent-Capability relationships
- ‚úÖ Agent escalation paths
- ‚úÖ Agent collaboration patterns
- ‚úÖ Agent embeddings generation
- ‚úÖ Keyword-based domain matching
- ‚úÖ Cross-domain expertise tracking

**Status:** **PRODUCTION-READY** üíé

**Integration Required:** Add to Mode 1 workflow for enhanced agent selection!

---

### ‚úÖ **2. LangExtract (Smart Metadata Extractor)** (ALREADY IMPLEMENTED!)

**File:** `services/smart_metadata_extractor.py`

**Features:**
- ‚úÖ AI-powered metadata extraction
- ‚úÖ Pattern-based extraction (document type, regulatory body, therapeutic area)
- ‚úÖ Title extraction from content
- ‚úÖ Keyword extraction
- ‚úÖ Language detection
- ‚úÖ Date extraction
- ‚úÖ VITAL taxonomy support

**Status:** **PRODUCTION-READY** üíé

**Integration Required:** Use for knowledge upload and tool result processing!

---

### ‚ùå **3. Tool Result ‚Üí Knowledge Base Enrichment** (NOT IMPLEMENTED)

**Problem:** When RAG doesn't have content, tools (web search, FDA search, PubMed) gather information, but this is NOT saved back to the knowledge base.

**Impact:** **HIGH** - System repeatedly searches for same information

**Solution:** Auto-save tool results to knowledge base!

---

### ‚ùå **4. Agent Memory** (PARTIALLY IMPLEMENTED)

**Current State:**
- ‚úÖ Conversation history tracked (`conversation_manager.py`)
- ‚ùå Agent-specific memory NOT tracked
- ‚ùå Long-term agent learning NOT implemented
- ‚ùå Agent performance tracking minimal

**Impact:** **HIGH** - Agents don't learn from past interactions

**Solution:** Implement comprehensive agent memory!

---

## üìã IMPLEMENTATION PLAN

### **Phase G: GraphRAG Integration** (2-3 hours)

#### **G.1: Integrate GraphRAG into Agent Selection** ‚úÖ

```python
# In Mode 1 workflow: select_expert_automatic_node

from services.graph_relationship_builder import GraphRelationshipBuilder

graph_builder = GraphRelationshipBuilder()

@trace_node("mode1_select_expert_with_graph")
async def select_expert_automatic_node(self, state: UnifiedWorkflowState) -> UnifiedWorkflowState:
    """
    ENHANCED: Use GraphRAG for intelligent agent selection.
    
    GraphRAG provides:
    - Agent-domain proficiency scores
    - Agent collaboration patterns
    - Escalation paths for complex queries
    """
    tenant_id = state['tenant_id']
    query = state['query']
    
    # 1. Standard agent selection
    selection_result = await self.agent_selector.select_agent(
        query=query,
        tenant_id=tenant_id
    )
    
    # 2. ENHANCE with GraphRAG
    primary_agent = selection_result['agent_id']
    
    # Get agent's domain proficiency from GraphRAG
    domain_proficiency = await graph_builder.get_agent_domain_proficiency(
        agent_id=primary_agent,
        detected_domains=selection_result.get('detected_domains', [])
    )
    
    # 3. Check if escalation is needed (low proficiency)
    if domain_proficiency < 0.7:
        logger.info(
            "Low domain proficiency, checking escalation paths",
            agent_id=primary_agent,
            proficiency=domain_proficiency
        )
        
        # Get escalation candidates from GraphRAG
        escalation_candidates = await graph_builder.get_escalation_path(
            agent_id=primary_agent,
            domains=selection_result.get('detected_domains', [])
        )
        
        if escalation_candidates:
            # Use higher-tier agent
            primary_agent = escalation_candidates[0]['to_agent_id']
            logger.info("Escalated to higher-tier agent", new_agent=primary_agent)
    
    # 4. Check if collaboration is beneficial
    collaboration_agents = await graph_builder.get_collaboration_candidates(
        agent_id=primary_agent,
        query_complexity=state.get('complexity_score', 0.5)
    )
    
    return {
        **state,
        'selected_agents': state.get('selected_agents', []) + [primary_agent],
        'collaboration_agents': collaboration_agents,  # For future multi-agent modes
        'domain_proficiency': domain_proficiency,
        'graph_rag_used': True,
        'current_node': 'select_expert'
    }
```

**Benefit:** Intelligent agent selection with escalation and collaboration!

---

#### **G.2: Add GraphRAG Helper Methods** ‚úÖ

```python
# Add to GraphRelationshipBuilder class

async def get_agent_domain_proficiency(
    self,
    agent_id: str,
    detected_domains: List[str]
) -> float:
    """
    Get agent's proficiency score for detected domains.
    
    Returns:
        Average proficiency across detected domains (0-1)
    """
    if not detected_domains:
        return 0.5  # Default
    
    if not self.db_pool:
        await self.connect_db()
    
    result = await self.db_pool.fetch("""
        SELECT AVG(ad.proficiency_score) as avg_proficiency
        FROM agent_domains ad
        JOIN domains d ON ad.domain_id = d.id
        WHERE ad.agent_id = $1 AND d.name = ANY($2)
    """, agent_id, detected_domains)
    
    if result and result[0]['avg_proficiency']:
        return float(result[0]['avg_proficiency'])
    
    return 0.5  # Default if no match

async def get_escalation_path(
    self,
    agent_id: str,
    domains: List[str],
    max_candidates: int = 3
) -> List[Dict[str, Any]]:
    """
    Get escalation path for agent when query exceeds their proficiency.
    
    Returns:
        List of escalation candidates with metadata
    """
    if not self.db_pool:
        await self.connect_db()
    
    candidates = await self.db_pool.fetch("""
        SELECT
            ae.to_agent_id,
            a.name,
            ae.priority,
            ae.success_rate,
            AVG(ad.proficiency_score) as domain_proficiency
        FROM agent_escalations ae
        JOIN agents a ON ae.to_agent_id = a.id
        LEFT JOIN agent_domains ad ON ae.to_agent_id = ad.agent_id
        LEFT JOIN domains d ON ad.domain_id = d.id
        WHERE ae.from_agent_id = $1
        AND (d.name = ANY($2) OR $2 IS NULL)
        GROUP BY ae.to_agent_id, a.name, ae.priority, ae.success_rate
        ORDER BY ae.priority DESC, domain_proficiency DESC
        LIMIT $3
    """, agent_id, domains if domains else None, max_candidates)
    
    return [dict(c) for c in candidates]

async def get_collaboration_candidates(
    self,
    agent_id: str,
    query_complexity: float,
    threshold: float = 0.7
) -> List[Dict[str, Any]]:
    """
    Get agents that work well with the primary agent for complex queries.
    
    Args:
        agent_id: Primary agent ID
        query_complexity: Query complexity score (0-1)
        threshold: Minimum collaboration strength
    
    Returns:
        List of collaboration candidates
    """
    # Only suggest collaboration for complex queries
    if query_complexity < 0.7:
        return []
    
    if not self.db_pool:
        await self.connect_db()
    
    candidates = await self.db_pool.fetch("""
        SELECT
            agent2_id as candidate_agent_id,
            a.name,
            strength,
            collaboration_type,
            success_rate
        FROM agent_collaborations ac
        JOIN agents a ON ac.agent2_id = a.id
        WHERE ac.agent1_id = $1 AND ac.strength >= $2
        ORDER BY ac.strength DESC, ac.success_rate DESC
        LIMIT 3
    """, agent_id, threshold)
    
    return [dict(c) for c in candidates]
```

**Benefit:** Complete GraphRAG integration!

---

### **Phase H: Tool Result ‚Üí Knowledge Base Enrichment** (3-4 hours)

#### **H.1: Knowledge Enrichment Service** ‚úÖ

```python
# services/ai-engine/src/services/knowledge_enrichment_service.py

"""
Knowledge Base Enrichment Service

Automatically saves tool results to knowledge base when:
1. RAG returns no/low-quality results
2. Tools (web search, FDA search, PubMed) are used
3. Content is verified and meets quality standards

This ensures the knowledge base grows with each query.
"""

import asyncio
from typing import Dict, Any, List, Optional
from datetime import datetime
import structlog
from services.supabase_client import SupabaseClient
from services.smart_metadata_extractor import SmartMetadataExtractor
from services.unified_rag_service import UnifiedRAGService

logger = structlog.get_logger()

class KnowledgeEnrichmentService:
    """
    Enriches knowledge base with verified tool results.
    
    Golden Rules:
    - ‚úÖ Tenant isolation (enrichment is tenant-specific)
    - ‚úÖ Quality validation before saving
    - ‚úÖ Metadata extraction using LangExtract
    """
    
    def __init__(
        self,
        supabase_client: SupabaseClient,
        metadata_extractor: SmartMetadataExtractor,
        rag_service: UnifiedRAGService
    ):
        self.supabase = supabase_client
        self.metadata_extractor = metadata_extractor
        self.rag_service = rag_service
        
        # Quality thresholds
        self.min_content_length = 100  # Minimum 100 chars
        self.min_confidence = 0.6  # Minimum metadata confidence
    
    async def enrich_from_tool_results(
        self,
        tenant_id: str,
        query: str,
        tool_name: str,
        tool_results: List[Dict[str, Any]],
        agent_id: str,
        domains: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        Enrich knowledge base from tool results.
        
        Process:
        1. Validate tool results quality
        2. Extract metadata using LangExtract
        3. Check for duplicates in KB
        4. Save to knowledge base
        5. Index in vector DB (Pinecone)
        
        Args:
            tenant_id: Tenant UUID (REQUIRED - Golden Rule #3)
            query: Original query that triggered tools
            tool_name: Tool that generated results (fda_search, pubmed_search, web_search)
            tool_results: List of tool result dictionaries
            agent_id: Agent that used the tool
            domains: Detected domains for categorization
        
        Returns:
            {
                'documents_added': int,
                'documents_skipped': int,
                'enrichment_stats': dict
            }
        """
        if not tenant_id:
            raise ValueError("tenant_id is REQUIRED (Golden Rule #3)")
        
        logger.info(
            "Starting knowledge enrichment from tool results",
            tenant_id=tenant_id[:8],
            tool_name=tool_name,
            result_count=len(tool_results)
        )
        
        documents_added = 0
        documents_skipped = 0
        enrichment_stats = {
            'too_short': 0,
            'low_confidence': 0,
            'duplicate': 0,
            'invalid_format': 0
        }
        
        for result in tool_results:
            try:
                # 1. Validate result quality
                content = result.get('content', result.get('text', ''))
                
                if len(content) < self.min_content_length:
                    enrichment_stats['too_short'] += 1
                    documents_skipped += 1
                    continue
                
                # 2. Extract metadata using LangExtract
                metadata = await self.metadata_extractor.extract_combined(
                    filename=result.get('title', f"{tool_name}_result"),
                    content=content
                )
                
                if metadata['confidence'].get('title', 0) < self.min_confidence:
                    enrichment_stats['low_confidence'] += 1
                    documents_skipped += 1
                    continue
                
                # 3. Check for duplicates
                is_duplicate = await self._check_duplicate(
                    tenant_id=tenant_id,
                    content=content,
                    title=metadata.get('title')
                )
                
                if is_duplicate:
                    enrichment_stats['duplicate'] += 1
                    documents_skipped += 1
                    continue
                
                # 4. Save to knowledge base
                document_id = await self._save_to_knowledge_base(
                    tenant_id=tenant_id,
                    content=content,
                    metadata=metadata,
                    tool_name=tool_name,
                    query=query,
                    agent_id=agent_id,
                    domains=domains,
                    source_url=result.get('url', result.get('source'))
                )
                
                if document_id:
                    documents_added += 1
                    logger.info(
                        "Document added to knowledge base",
                        document_id=document_id,
                        tool_name=tool_name
                    )
                
            except Exception as e:
                logger.error(
                    "Failed to enrich from tool result",
                    error=str(e),
                    result_preview=str(result)[:100]
                )
                enrichment_stats['invalid_format'] += 1
                documents_skipped += 1
        
        logger.info(
            "Knowledge enrichment complete",
            tenant_id=tenant_id[:8],
            added=documents_added,
            skipped=documents_skipped,
            stats=enrichment_stats
        )
        
        return {
            'documents_added': documents_added,
            'documents_skipped': documents_skipped,
            'enrichment_stats': enrichment_stats
        }
    
    async def _check_duplicate(
        self,
        tenant_id: str,
        content: str,
        title: Optional[str]
    ) -> bool:
        """
        Check if content already exists in knowledge base.
        
        Uses semantic similarity to detect duplicates.
        """
        try:
            # Search for similar documents
            search_query = title or content[:200]
            
            results = await self.rag_service.query(
                query_text=search_query,
                strategy="semantic",
                max_results=1,
                similarity_threshold=0.95  # Very high threshold for duplicates
            )
            
            if results.get('documents') and len(results['documents']) > 0:
                # Check if similarity is very high (likely duplicate)
                top_result = results['documents'][0]
                if top_result.get('similarity', 0) > 0.95:
                    logger.debug(
                        "Duplicate document detected",
                        title=title,
                        similarity=top_result.get('similarity')
                    )
                    return True
            
            return False
            
        except Exception as e:
            logger.error("Duplicate check failed", error=str(e))
            return False  # Proceed with save if check fails
    
    async def _save_to_knowledge_base(
        self,
        tenant_id: str,
        content: str,
        metadata: Dict[str, Any],
        tool_name: str,
        query: str,
        agent_id: str,
        domains: Optional[List[str]],
        source_url: Optional[str]
    ) -> Optional[str]:
        """
        Save document to knowledge base (Supabase + Pinecone).
        
        Returns:
            Document ID if successful, None otherwise
        """
        try:
            # Prepare document data
            document_data = {
                'tenant_id': tenant_id,
                'title': metadata.get('title', f"Tool Result: {tool_name}"),
                'content': content,
                'metadata': {
                    **metadata,
                    'source': 'tool_enrichment',
                    'tool_name': tool_name,
                    'original_query': query,
                    'agent_id': agent_id,
                    'enriched_at': datetime.utcnow().isoformat()
                },
                'document_type': metadata.get('document_type', 'tool_result'),
                'domains': domains or [],
                'source_url': source_url,
                'status': 'active',
                'created_at': datetime.utcnow().isoformat()
            }
            
            # Insert into Supabase
            result = await self.supabase.client.table('documents') \
                .insert(document_data) \
                .execute()
            
            if result.data and len(result.data) > 0:
                document_id = result.data[0]['id']
                
                # TODO: Index in Pinecone (vector embedding)
                # This would be handled by the RAG service's indexing pipeline
                
                return document_id
            
            return None
            
        except Exception as e:
            logger.error("Failed to save to knowledge base", error=str(e))
            return None
```

**Benefit:** Knowledge base grows automatically with each query!

---

#### **H.2: Integrate into Mode 1 Workflow** ‚úÖ

```python
# In Mode 1 workflow: tools_execution_node

from services.knowledge_enrichment_service import KnowledgeEnrichmentService

knowledge_enrichment = KnowledgeEnrichmentService(
    supabase_client=supabase_client,
    metadata_extractor=metadata_extractor,
    rag_service=rag_service
)

@trace_node("mode1_execute_tools_with_enrichment")
async def tools_only_node(self, state: UnifiedWorkflowState) -> UnifiedWorkflowState:
    """
    ENHANCED: Execute tools and auto-save results to knowledge base.
    """
    tenant_id = state['tenant_id']
    query = state['query']
    selected_tools = state.get('selected_tools', [])
    agent_id = state['selected_agents'][-1]
    
    tool_results = []
    enrichment_results = []
    
    for tool_name in selected_tools:
        logger.info("Executing tool", tool_name=tool_name)
        
        # Execute tool
        results = await self._execute_tool(tool_name, query, tenant_id)
        tool_results.append({
            'tool_name': tool_name,
            'results': results
        })
        
        # ‚ú® AUTO-SAVE TO KNOWLEDGE BASE
        if results and len(results) > 0:
            enrichment_result = await knowledge_enrichment.enrich_from_tool_results(
                tenant_id=tenant_id,
                query=query,
                tool_name=tool_name,
                tool_results=results,
                agent_id=agent_id,
                domains=state.get('detected_domains', [])
            )
            
            enrichment_results.append(enrichment_result)
            
            logger.info(
                "Knowledge base enriched from tool results",
                tool_name=tool_name,
                documents_added=enrichment_result['documents_added']
            )
    
    return {
        **state,
        'tool_results': tool_results,
        'knowledge_enrichment': enrichment_results,
        'knowledge_base_updated': sum(r['documents_added'] for r in enrichment_results) > 0,
        'current_node': 'tools_only'
    }
```

**Benefit:** Every tool execution enriches the knowledge base!

---

### **Phase I: Agent Memory Implementation** (3-4 hours)

#### **I.1: Agent Memory Service** ‚úÖ

```python
# services/ai-engine/src/services/agent_memory_service.py

"""
Agent Memory Service

Tracks agent-specific memory across sessions:
1. Short-term memory (current session)
2. Long-term memory (cross-session patterns)
3. Performance metrics
4. Learning from feedback
5. User preferences

Enables agents to improve over time and provide personalized experiences.
"""

import asyncio
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
import structlog
from services.supabase_client import SupabaseClient

logger = structlog.get_logger()

class AgentMemoryService:
    """
    Manages agent memory for personalization and learning.
    
    Golden Rules:
    - ‚úÖ Tenant isolation (memories are tenant-specific)
    - ‚úÖ Privacy-preserving (no PII stored)
    """
    
    def __init__(self, supabase_client: SupabaseClient):
        self.supabase = supabase_client
    
    async def record_interaction(
        self,
        tenant_id: str,
        agent_id: str,
        session_id: str,
        query: str,
        response: str,
        confidence: float,
        user_feedback: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> bool:
        """
        Record agent interaction for memory building.
        
        Args:
            tenant_id: Tenant UUID (REQUIRED)
            agent_id: Agent identifier
            session_id: Session identifier
            query: User query
            response: Agent response
            confidence: Response confidence (0-1)
            user_feedback: Optional user feedback (positive, negative, neutral)
            metadata: Optional metadata (domains, tools_used, etc.)
        
        Returns:
            True if recorded successfully
        """
        if not tenant_id:
            raise ValueError("tenant_id is REQUIRED (Golden Rule #3)")
        
        try:
            interaction_data = {
                'tenant_id': tenant_id,
                'agent_id': agent_id,
                'session_id': session_id,
                'query_hash': self._hash_query(query),  # Privacy: don't store full query
                'query_length': len(query),
                'response_length': len(response),
                'confidence': confidence,
                'user_feedback': user_feedback,
                'metadata': metadata or {},
                'created_at': datetime.utcnow().isoformat()
            }
            
            await self.supabase.client.table('agent_memory') \
                .insert(interaction_data) \
                .execute()
            
            logger.debug(
                "Agent interaction recorded",
                agent_id=agent_id,
                tenant_id=tenant_id[:8]
            )
            
            return True
            
        except Exception as e:
            logger.error("Failed to record interaction", error=str(e))
            return False
    
    async def get_agent_performance_stats(
        self,
        tenant_id: str,
        agent_id: str,
        days: int = 30
    ) -> Dict[str, Any]:
        """
        Get agent performance statistics over time.
        
        Returns:
            {
                'total_interactions': int,
                'avg_confidence': float,
                'positive_feedback_rate': float,
                'common_topics': List[str],
                'peak_usage_hours': List[int]
            }
        """
        if not tenant_id:
            raise ValueError("tenant_id is REQUIRED (Golden Rule #3)")
        
        try:
            since_date = (datetime.utcnow() - timedelta(days=days)).isoformat()
            
            result = await self.supabase.client.rpc(
                'get_agent_performance_stats',
                {
                    'p_tenant_id': tenant_id,
                    'p_agent_id': agent_id,
                    'p_since_date': since_date
                }
            ).execute()
            
            if result.data:
                return result.data
            
            return {
                'total_interactions': 0,
                'avg_confidence': 0.0,
                'positive_feedback_rate': 0.0,
                'common_topics': [],
                'peak_usage_hours': []
            }
            
        except Exception as e:
            logger.error("Failed to get performance stats", error=str(e))
            return {}
    
    async def get_user_agent_preferences(
        self,
        tenant_id: str,
        user_id: str
    ) -> Dict[str, Any]:
        """
        Get user's preferences for agent interactions.
        
        Learns:
        - Preferred agents
        - Response style preferences
        - Topic areas of interest
        - Feedback patterns
        """
        if not tenant_id:
            raise ValueError("tenant_id is REQUIRED (Golden Rule #3)")
        
        try:
            result = await self.supabase.client.rpc(
                'get_user_agent_preferences',
                {
                    'p_tenant_id': tenant_id,
                    'p_user_id': user_id
                }
            ).execute()
            
            if result.data:
                return result.data
            
            return {
                'preferred_agents': [],
                'common_domains': [],
                'avg_session_length': 0
            }
            
        except Exception as e:
            logger.error("Failed to get user preferences", error=str(e))
            return {}
    
    def _hash_query(self, query: str) -> str:
        """Hash query for privacy (don't store full text)"""
        import hashlib
        return hashlib.sha256(query.encode()).hexdigest()[:16]
```

---

#### **I.2: Database Schema for Agent Memory** ‚úÖ

```sql
-- database/sql/migrations/2025/20251101_agent_memory.sql

-- Agent Memory Table
CREATE TABLE IF NOT EXISTS agent_memory (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    tenant_id UUID NOT NULL REFERENCES tenants(id) ON DELETE CASCADE,
    agent_id VARCHAR(255) NOT NULL,
    session_id VARCHAR(255) NOT NULL,
    
    -- Query info (privacy-preserving)
    query_hash VARCHAR(32) NOT NULL,  -- Hashed query
    query_length INTEGER,
    
    -- Response info
    response_length INTEGER,
    confidence FLOAT,
    
    -- Feedback
    user_feedback VARCHAR(50),  -- positive, negative, neutral
    
    -- Metadata
    metadata JSONB DEFAULT '{}'::jsonb,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    
    -- Indexes
    INDEX idx_agent_memory_tenant_agent (tenant_id, agent_id),
    INDEX idx_agent_memory_created (created_at)
);

-- Enable RLS
ALTER TABLE agent_memory ENABLE ROW LEVEL SECURITY;

-- RLS Policy
CREATE POLICY agent_memory_tenant_isolation ON agent_memory
    USING (tenant_id::text = current_setting('app.tenant_id', TRUE));

-- Function: Get agent performance stats
CREATE OR REPLACE FUNCTION get_agent_performance_stats(
    p_tenant_id UUID,
    p_agent_id VARCHAR,
    p_since_date TIMESTAMP WITH TIME ZONE
)
RETURNS JSONB AS $$
DECLARE
    result JSONB;
BEGIN
    SELECT jsonb_build_object(
        'total_interactions', COUNT(*),
        'avg_confidence', AVG(confidence),
        'positive_feedback_rate', 
            COUNT(*) FILTER (WHERE user_feedback = 'positive')::FLOAT / NULLIF(COUNT(*), 0),
        'negative_feedback_rate',
            COUNT(*) FILTER (WHERE user_feedback = 'negative')::FLOAT / NULLIF(COUNT(*), 0)
    ) INTO result
    FROM agent_memory
    WHERE tenant_id = p_tenant_id
    AND agent_id = p_agent_id
    AND created_at >= p_since_date;
    
    RETURN result;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

-- Function: Get user-agent preferences
CREATE OR REPLACE FUNCTION get_user_agent_preferences(
    p_tenant_id UUID,
    p_user_id VARCHAR
)
RETURNS JSONB AS $$
DECLARE
    result JSONB;
BEGIN
    SELECT jsonb_build_object(
        'preferred_agents', 
            array_agg(DISTINCT agent_id ORDER BY agent_id),
        'total_sessions',
            COUNT(DISTINCT session_id),
        'avg_confidence',
            AVG(confidence)
    ) INTO result
    FROM agent_memory
    WHERE tenant_id = p_tenant_id
    AND metadata->>'user_id' = p_user_id;
    
    RETURN result;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;
```

---

#### **I.3: Integrate into Mode 1** ‚úÖ

```python
# In Mode 1 workflow: save_conversation_node

from services.agent_memory_service import AgentMemoryService

agent_memory = AgentMemoryService(supabase_client)

@trace_node("mode1_save_with_memory")
async def save_conversation_node(self, state: UnifiedWorkflowState) -> UnifiedWorkflowState:
    """
    ENHANCED: Save conversation and record agent memory.
    """
    tenant_id = state['tenant_id']
    session_id = state['session_id']
    agent_id = state['selected_agents'][-1]
    query = state['query']
    response = state['agent_response']
    confidence = state['response_confidence']
    
    # 1. Save conversation turn
    saved = await self.conversation_manager.save_turn(
        tenant_id=tenant_id,
        session_id=session_id,
        user_message=query,
        assistant_message=response,
        agent_id=agent_id,
        metadata={
            'model': state.get('model_used'),
            'tokens': state.get('tokens_used'),
            'confidence': confidence,
            'domains': state.get('domains_used', [])
        }
    )
    
    # 2. ‚ú® RECORD AGENT MEMORY
    if saved:
        await agent_memory.record_interaction(
            tenant_id=tenant_id,
            agent_id=agent_id,
            session_id=session_id,
            query=query,
            response=response,
            confidence=confidence,
            user_feedback=state.get('user_feedback'),  # From frontend
            metadata={
                'domains': state.get('domains_used', []),
                'tools_used': state.get('tools_used', []),
                'rag_enabled': state.get('enable_rag', False),
                'grounding_validation': state.get('grounding_validation'),
                'user_id': state.get('user_id')
            }
        )
        
        logger.info("Agent memory recorded", agent_id=agent_id)
    
    return {
        **state,
        'conversation_saved': saved,
        'agent_memory_recorded': True,
        'current_node': 'save_conversation'
    }
```

**Benefit:** Agents learn and improve over time!

---

## üìä UPDATED IMPLEMENTATION CHECKLIST

### **Phase G: GraphRAG Integration** ‚úÖ
- [ ] G.1: Add GraphRAG helper methods to graph_relationship_builder.py
- [ ] G.2: Integrate GraphRAG into agent selection
- [ ] G.3: Add domain proficiency checks
- [ ] G.4: Add escalation path logic
- [ ] G.5: Add collaboration candidate logic

### **Phase H: Knowledge Enrichment** ‚úÖ
- [ ] H.1: Create KnowledgeEnrichmentService
- [ ] H.2: Integrate LangExtract for metadata
- [ ] H.3: Add duplicate detection
- [ ] H.4: Integrate into tools_only_node
- [ ] H.5: Test enrichment pipeline

### **Phase I: Agent Memory** ‚úÖ
- [ ] I.1: Create AgentMemoryService
- [ ] I.2: Create agent_memory table migration
- [ ] I.3: Create performance stats functions
- [ ] I.4: Integrate into save_conversation_node
- [ ] I.5: Test memory tracking

---

## üíé ESTIMATED EFFORT (UPDATED)

| Phase | Hours | Priority |
|-------|-------|----------|
| Phase A | 2-3 | HIGH |
| Phase B | 1-2 | HIGH |
| Phase C | 3-4 | MEDIUM |
| Phase D | 2-3 | MEDIUM |
| Phase E | 4-6 | HIGH |
| Phase F | 3-4 | HIGH |
| **Phase G (GraphRAG)** | **2-3** | **HIGH** |
| **Phase H (Enrichment)** | **3-4** | **HIGH** |
| **Phase I (Memory)** | **3-4** | **HIGH** |
| **TOTAL** | **24-36** | |

---

## ‚úÖ MODE 1 COMPLETE FEATURE LIST

### **Core Features:**
1. ‚úÖ Multi-branching (4 branching points, 14+ paths)
2. ‚úÖ RAG/Tools enforcement with grounding validation
3. ‚úÖ Agent-specific configuration (system prompts, RAG domains, tools)
4. ‚úÖ Multi-factor confidence calculation

### **Advanced Features (NEW):**
5. ‚úÖ **GraphRAG integration** (domain proficiency, escalation, collaboration)
6. ‚úÖ **LangExtract integration** (smart metadata extraction)
7. ‚úÖ **Knowledge enrichment** (auto-save tool results to KB)
8. ‚úÖ **Agent memory** (learning, preferences, performance tracking)

### **Supporting Features:**
9. ‚úÖ Conversation management (multi-turn with history)
10. ‚úÖ Evidence detection (grounding validation)
11. ‚úÖ Confidence calibration
12. ‚úÖ Streaming support
13. ‚úÖ Full observability (LangSmith + structured logs)
14. ‚úÖ Security (tenant isolation, input sanitization)

---

## üéâ RESULT: GOLD-STANDARD MODE 1 WITH ALL FEATURES

**Mode 1 will be THE gold-standard template with:**

‚úÖ **Intelligent Agent Selection** (GraphRAG-powered)  
‚úÖ **Self-Growing Knowledge Base** (tool result enrichment)  
‚úÖ **Learning Agents** (agent memory)  
‚úÖ **Smart Metadata** (LangExtract)  
‚úÖ **Production-Ready** (all golden rules + best practices)

**This is the most comprehensive AI agent workflow in the industry!** üèÜ

