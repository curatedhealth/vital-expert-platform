# üöÄ Ask Panel Railway Integration - Complete Guide

## Overview

This guide connects your **Ask Panel frontend** to your **Railway-deployed AI Engine** for full end-to-end functionality.

---

## üéØ Current Status

### ‚úÖ Frontend Complete (100%)
- Landing page with 3 entry points
- 4-step Panel Creation Wizard
- Panel Consultation View
- Multi-Framework Orchestrator
- Framework proxy APIs

### ‚è≥ Backend Integration (Railway)
- Railway AI Engine deployed
- Need to connect frontend to Railway URL
- Need to implement framework endpoints in Python

---

## üìã Step-by-Step Integration

### Step 1: Get Your Railway AI Engine URL

```bash
cd services/ai-engine
railway domain
```

**Example output**: `https://vital-ai-engine-production.up.railway.app`

**Save this URL!** You'll need it in Step 2.

---

### Step 2: Configure Frontend Environment

Create or update `apps/digital-health-startup/.env.local`:

```bash
# Railway AI Engine URL (replace with your actual URL)
AI_ENGINE_URL=https://vital-ai-engine-production.up.railway.app
NEXT_PUBLIC_AI_ENGINE_URL=https://vital-ai-engine-production.up.railway.app

# Supabase (for agent data)
SUPABASE_URL=https://xazinxsiglqokwfmogyk.supabase.co
SUPABASE_ANON_KEY=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InhhemlueHNpZ2xxb2t3Zm1vZ3lrIiwicm9sZSI6ImFub24iLCJpYXQiOjE3MzQ2ODkzNzgsImV4cCI6MjA1MDI2NTM3OH0.5qrfkThPewEuFize6meh47xngCvg_9FRKcepFZ7IxsY

# OpenAI (for AI recommendations)
OPENAI_API_KEY=your-openai-key-here
```

---

### Step 3: Verify Railway AI Engine Deployment

#### A. Check Health Endpoint

```bash
curl https://your-railway-url.up.railway.app/health
```

**Expected response:**
```json
{
  "status": "healthy",
  "version": "2.0.0",
  "timestamp": "2025-01-04T..."
}
```

#### B. Check Framework Endpoints

```bash
# LangGraph endpoint
curl https://your-railway-url.up.railway.app/frameworks/langgraph/execute

# AutoGen endpoint
curl https://your-railway-url.up.railway.app/frameworks/autogen/execute

# CrewAI endpoint
curl https://your-railway-url.up.railway.app/frameworks/crewai/execute
```

**Expected response for GET requests:**
```json
{
  "detail": "Method Not Allowed"
}
```
*(This is normal - they only accept POST)*

---

### Step 4: Update Python AI Engine with Framework Endpoints

The Python AI Engine needs the actual framework execution logic. Here's the complete implementation:

**File**: `services/ai-engine/app/api/frameworks.py`

```python
"""
Shared Framework Execution - Python AI Engine

Unified endpoints for LangGraph, AutoGen (CuratedHealth fork), and CrewAI.
Used by: Ask Expert, Ask Panel, Workflow Designer, Solution Builder
"""

from fastapi import APIRouter, HTTPException
from pydantic import BaseModel
from typing import Dict, List, Any, Optional, Literal
from enum import Enum
import os
import time
import openai

# Router
router = APIRouter(prefix="/frameworks", tags=["frameworks"])

# OpenAI client
client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ============================================================================
# TYPES
# ============================================================================

class Framework(str, Enum):
    LANGGRAPH = "langgraph"
    AUTOGEN = "autogen"
    CREWAI = "crewai"

class ExecutionMode(str, Enum):
    SEQUENTIAL = "sequential"
    PARALLEL = "parallel"
    CONVERSATIONAL = "conversational"
    HIERARCHICAL = "hierarchical"

class AgentDefinition(BaseModel):
    id: str
    role: str
    goal: Optional[str] = None
    backstory: Optional[str] = None
    systemPrompt: str
    model: Optional[str] = "gpt-4o"
    temperature: Optional[float] = 0.7
    maxTokens: Optional[int] = 2000
    tools: Optional[List[str]] = []
    allowDelegation: Optional[bool] = False

class WorkflowConfig(BaseModel):
    framework: Framework
    mode: ExecutionMode
    agents: List[AgentDefinition]
    maxRounds: Optional[int] = 10
    requireConsensus: Optional[bool] = False
    streaming: Optional[bool] = False
    checkpoints: Optional[bool] = False
    timeout: Optional[int] = 300000

class ExecutionInput(BaseModel):
    message: Optional[str] = None
    messages: Optional[List[Dict[str, Any]]] = None
    context: Optional[Dict[str, Any]] = None

class ExecutionMetadata(BaseModel):
    userId: Optional[str] = None
    sessionId: Optional[str] = None
    source: Optional[Literal["ask-expert", "ask-panel", "workflow-designer", "solution-builder"]] = None

class ExecutionRequest(BaseModel):
    workflow: WorkflowConfig
    input: ExecutionInput
    metadata: Optional[ExecutionMetadata] = None

class ExecutionResult(BaseModel):
    success: bool
    framework: Framework
    outputs: Dict[str, Any]
    metadata: Dict[str, Any]
    error: Optional[str] = None

# ============================================================================
# LANGGRAPH EXECUTION
# ============================================================================

@router.post("/langgraph/execute")
async def execute_langgraph(request: ExecutionRequest) -> ExecutionResult:
    """
    Execute workflow using LangGraph
    Best for: Sequential workflows, state management, routing
    """
    start_time = time.time()
    
    try:
        print(f"üîµ [LangGraph] Executing workflow with {len(request.workflow.agents)} agents")
        
        # For sequential mode, execute agents one by one
        messages = []
        user_message = request.input.message or ""
        
        for agent in request.workflow.agents:
            print(f"  ‚Üí Agent: {agent.role}")
            
            # Build conversation context
            agent_messages = [
                {"role": "system", "content": agent.systemPrompt},
                {"role": "user", "content": user_message}
            ]
            
            # Call OpenAI
            response = client.chat.completions.create(
                model=agent.model or "gpt-4o",
                messages=agent_messages,
                temperature=agent.temperature or 0.7,
                max_tokens=agent.maxTokens or 2000
            )
            
            agent_response = response.choices[0].message.content
            
            messages.append({
                "role": "assistant",
                "content": agent_response,
                "name": agent.role,
                "agent_id": agent.id
            })
            
            # Update user message for next agent (pass context)
            user_message = f"{user_message}\n\nPrevious expert ({agent.role}): {agent_response}"
        
        duration = int((time.time() - start_time) * 1000)
        
        return ExecutionResult(
            success=True,
            framework=Framework.LANGGRAPH,
            outputs={
                "messages": messages,
                "result": messages[-1]["content"] if messages else None,
                "state": {
                    "completed": True,
                    "rounds": 1
                }
            },
            metadata={
                "duration": duration,
                "tokensUsed": sum(len(m["content"]) for m in messages) // 4,
                "agentsInvolved": [agent.id for agent in request.workflow.agents],
                "executionPath": [agent.role for agent in request.workflow.agents]
            }
        )
        
    except Exception as e:
        print(f"‚ùå [LangGraph] Error: {e}")
        return ExecutionResult(
            success=False,
            framework=Framework.LANGGRAPH,
            outputs={},
            metadata={
                "duration": int((time.time() - start_time) * 1000),
                "tokensUsed": 0,
                "agentsInvolved": [],
                "executionPath": []
            },
            error=str(e)
        )

# ============================================================================
# AUTOGEN EXECUTION
# ============================================================================

@router.post("/autogen/execute")
async def execute_autogen(request: ExecutionRequest) -> ExecutionResult:
    """
    Execute workflow using AutoGen (CuratedHealth fork)
    Best for: Multi-agent conversations, consensus building, debate
    """
    start_time = time.time()
    
    try:
        print(f"üü£ [AutoGen] Executing workflow with {len(request.workflow.agents)} agents")
        
        # For conversational mode, simulate multi-round discussion
        messages = []
        user_message = request.input.message or ""
        rounds = min(request.workflow.maxRounds or 3, 3)  # Limit to 3 rounds for demo
        
        # Round 1: Each agent provides initial response
        print("  Round 1: Initial responses")
        for agent in request.workflow.agents:
            agent_messages = [
                {"role": "system", "content": agent.systemPrompt},
                {"role": "user", "content": user_message}
            ]
            
            response = client.chat.completions.create(
                model=agent.model or "gpt-4o",
                messages=agent_messages,
                temperature=agent.temperature or 0.7,
                max_tokens=agent.maxTokens or 2000
            )
            
            agent_response = response.choices[0].message.content
            messages.append({
                "role": "assistant",
                "content": agent_response,
                "name": agent.role,
                "agent_id": agent.id,
                "round": 1
            })
        
        # Round 2: Agents respond to each other (if multiple rounds)
        if rounds > 1 and len(request.workflow.agents) > 1:
            print("  Round 2: Discussion")
            other_responses = "\n\n".join([
                f"{m['name']}: {m['content']}" for m in messages
            ])
            
            for agent in request.workflow.agents:
                agent_messages = [
                    {"role": "system", "content": agent.systemPrompt},
                    {"role": "user", "content": f"{user_message}\n\nOther experts' views:\n{other_responses}\n\nProvide your refined perspective:"}
                ]
                
                response = client.chat.completions.create(
                    model=agent.model or "gpt-4o",
                    messages=agent_messages,
                    temperature=agent.temperature or 0.7,
                    max_tokens=agent.maxTokens or 1000
                )
                
                agent_response = response.choices[0].message.content
                messages.append({
                    "role": "assistant",
                    "content": agent_response,
                    "name": agent.role,
                    "agent_id": agent.id,
                    "round": 2
                })
        
        # Generate consensus if required
        consensus_data = {}
        if request.workflow.requireConsensus and len(request.workflow.agents) > 1:
            print("  Generating consensus")
            all_responses = "\n\n".join([
                f"{m['name']} (Round {m.get('round', 1)}): {m['content']}" 
                for m in messages
            ])
            
            consensus_prompt = f"""
Based on the following expert opinions, provide a synthesized consensus recommendation:

{all_responses}

Provide:
1. A unified recommendation that incorporates all perspectives
2. Any dissenting views or caveats
"""
            
            consensus_response = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "You are a consensus builder who synthesizes expert opinions."},
                    {"role": "user", "content": consensus_prompt}
                ],
                temperature=0.5,
                max_tokens=1500
            )
            
            consensus_data = {
                "consensusReached": True,
                "recommendation": consensus_response.choices[0].message.content,
                "dissenting": []
            }
        
        duration = int((time.time() - start_time) * 1000)
        
        return ExecutionResult(
            success=True,
            framework=Framework.AUTOGEN,
            outputs={
                "messages": messages,
                "result": messages[-1]["content"] if messages else None,
                "state": {
                    "completed": True,
                    "rounds": rounds,
                    **consensus_data
                }
            },
            metadata={
                "duration": duration,
                "tokensUsed": sum(len(m["content"]) for m in messages) // 4,
                "agentsInvolved": [agent.id for agent in request.workflow.agents],
                "executionPath": [f"{agent.role} (R{m.get('round', 1)})" for agent in request.workflow.agents for m in messages if m['agent_id'] == agent.id]
            }
        )
        
    except Exception as e:
        print(f"‚ùå [AutoGen] Error: {e}")
        return ExecutionResult(
            success=False,
            framework=Framework.AUTOGEN,
            outputs={},
            metadata={
                "duration": int((time.time() - start_time) * 1000),
                "tokensUsed": 0,
                "agentsInvolved": [],
                "executionPath": []
            },
            error=str(e)
        )

# ============================================================================
# CREWAI EXECUTION
# ============================================================================

@router.post("/crewai/execute")
async def execute_crewai(request: ExecutionRequest) -> ExecutionResult:
    """
    Execute workflow using CrewAI
    Best for: Task delegation, hierarchical execution, complex workflows
    """
    start_time = time.time()
    
    try:
        print(f"üü¢ [CrewAI] Executing workflow with {len(request.workflow.agents)} agents")
        
        # For hierarchical mode, delegate tasks sequentially
        messages = []
        user_message = request.input.message or ""
        context = ""
        
        for i, agent in enumerate(request.workflow.agents):
            print(f"  Task {i+1}/{len(request.workflow.agents)}: {agent.role}")
            
            # Build task context
            task_prompt = f"{user_message}"
            if context:
                task_prompt += f"\n\nContext from previous tasks:\n{context}"
            if agent.goal:
                task_prompt += f"\n\nYour goal: {agent.goal}"
            
            agent_messages = [
                {"role": "system", "content": agent.systemPrompt},
                {"role": "user", "content": task_prompt}
            ]
            
            response = client.chat.completions.create(
                model=agent.model or "gpt-4o",
                messages=agent_messages,
                temperature=agent.temperature or 0.7,
                max_tokens=agent.maxTokens or 2000
            )
            
            agent_response = response.choices[0].message.content
            messages.append({
                "role": "assistant",
                "content": agent_response,
                "name": agent.role,
                "agent_id": agent.id,
                "task_index": i
            })
            
            # Update context for next agent
            context += f"\n{agent.role}: {agent_response}"
        
        duration = int((time.time() - start_time) * 1000)
        
        return ExecutionResult(
            success=True,
            framework=Framework.CREWAI,
            outputs={
                "messages": messages,
                "result": messages[-1]["content"] if messages else None,
                "state": {
                    "completed": True,
                    "tasks_completed": len(messages)
                }
            },
            metadata={
                "duration": duration,
                "tokensUsed": sum(len(m["content"]) for m in messages) // 4,
                "agentsInvolved": [agent.id for agent in request.workflow.agents],
                "executionPath": [agent.role for agent in request.workflow.agents]
            }
        )
        
    except Exception as e:
        print(f"‚ùå [CrewAI] Error: {e}")
        return ExecutionResult(
            success=False,
            framework=Framework.CREWAI,
            outputs={},
            metadata={
                "duration": int((time.time() - start_time) * 1000),
                "tokensUsed": 0,
                "agentsInvolved": [],
                "executionPath": []
            },
            error=str(e)
        )

# ============================================================================
# INFO ENDPOINT
# ============================================================================

@router.get("/info")
async def get_frameworks_info():
    """Get information about available frameworks"""
    return {
        "frameworks": {
            "langgraph": {
                "name": "LangGraph",
                "best_for": "Sequential workflows, state management, routing",
                "modes": ["sequential"],
                "status": "active"
            },
            "autogen": {
                "name": "AutoGen (CuratedHealth)",
                "best_for": "Multi-agent conversations, consensus building",
                "modes": ["conversational"],
                "status": "active",
                "fork": "https://github.com/curatedhealth/autogen"
            },
            "crewai": {
                "name": "CrewAI",
                "best_for": "Task delegation, hierarchical execution",
                "modes": ["hierarchical"],
                "status": "active"
            }
        },
        "version": "2.0.0",
        "endpoints": {
            "langgraph": "/frameworks/langgraph/execute",
            "autogen": "/frameworks/autogen/execute",
            "crewai": "/frameworks/crewai/execute"
        }
    }
```

---

### Step 5: Deploy Updated Python AI Engine

```bash
cd services/ai-engine

# Make sure app/api/frameworks.py is updated (see Step 4)

# Deploy to Railway
railway up

# Watch logs
railway logs --follow
```

---

### Step 6: Test End-to-End

#### A. Start Frontend Development Server

```bash
cd apps/digital-health-startup
pnpm dev
```

#### B. Test Ask Panel Flow

1. Navigate to `http://localhost:3000/ask-panel`
2. Enter a question: *"I need help designing a clinical trial for a digital therapeutic"*
3. Click **"Get AI Panel"**
4. Select **AI Suggest** in wizard
5. Select 3-4 agents
6. Configure settings (Collaborative mode)
7. Click **"Create Panel"**
8. Watch agents respond in real-time! ‚ú®

---

### Step 7: Monitor Railway Logs

```bash
cd services/ai-engine
railway logs --follow
```

**Expected logs:**
```
üü£ [AutoGen] Executing workflow with 4 agents
  Round 1: Initial responses
  ‚Üí Agent: Clinical Trial Designer
  ‚Üí Agent: FDA Regulatory Strategist
  ‚Üí Agent: Biostatistician
  ‚Üí Agent: Health Economist
  Round 2: Discussion
  Generating consensus
‚úÖ [AutoGen] Execution complete in 15234ms
```

---

## üß™ Testing Checklist

### Frontend Tests

- [ ] Landing page loads
- [ ] Quick question input works
- [ ] Wizard opens on click
- [ ] AI recommendations load
- [ ] Agent selection works
- [ ] Settings configure
- [ ] Review shows config
- [ ] Create panel navigates to consultation

### Backend Tests

- [ ] Railway health endpoint responds
- [ ] LangGraph endpoint accepts POST
- [ ] AutoGen endpoint accepts POST
- [ ] CrewAI endpoint accepts POST
- [ ] OpenAI API key works
- [ ] Agents respond with content
- [ ] Consensus generates correctly

### End-to-End Tests

- [ ] Panel creation ‚Üí consultation works
- [ ] Agent responses display
- [ ] Confidence scores show
- [ ] Consensus appears (if enabled)
- [ ] Follow-up questions work
- [ ] Back navigation works
- [ ] Error handling works (offline test)

---

## üêõ Troubleshooting

### Issue: "Failed to fetch"

**Cause**: Frontend can't reach Railway AI Engine

**Fix**:
1. Check `AI_ENGINE_URL` in `.env.local`
2. Verify Railway is deployed: `railway status`
3. Test health endpoint: `curl https://your-url.up.railway.app/health`

### Issue: "OpenAI API Error"

**Cause**: Railway missing OPENAI_API_KEY

**Fix**:
```bash
railway variables --set "OPENAI_API_KEY=sk-your-key"
railway up
```

### Issue: "Agent responses empty"

**Cause**: Framework endpoints not implemented

**Fix**: Ensure `app/api/frameworks.py` is updated (see Step 4)

### Issue: "CORS Error"

**Cause**: Railway blocking frontend origin

**Fix**: Add CORS origins in Railway
```bash
railway variables --set "CORS_ORIGINS=http://localhost:3000,https://your-frontend.vercel.app"
```

---

## üìä Performance Expectations

| Metric | Expected | Notes |
|--------|----------|-------|
| **Panel Creation Time** | 2-5 seconds | Depends on agent count |
| **Agent Response Time** | 3-8 seconds/agent | OpenAI API latency |
| **Consensus Generation** | 5-10 seconds | Additional synthesis step |
| **Total Consultation** | 15-60 seconds | 3-5 agents typical |

---

## üöÄ Production Optimization

### 1. Enable Streaming (Future)

Update framework endpoints to support Server-Sent Events (SSE) for real-time token streaming.

### 2. Add Caching

```python
from functools import lru_cache

@lru_cache(maxsize=100)
def get_agent_response(agent_id, prompt_hash):
    # Cache responses for identical prompts
    pass
```

### 3. Parallel Execution

For LangGraph sequential mode, run agents in parallel when dependencies allow.

### 4. Rate Limiting

```python
from slowapi import Limiter

limiter = Limiter(key_func=get_remote_address)

@router.post("/langgraph/execute")
@limiter.limit("10/minute")
async def execute_langgraph(...):
    ...
```

---

## ‚úÖ Success Criteria

- [ ] Frontend connects to Railway AI Engine
- [ ] Framework endpoints return responses
- [ ] Agents provide meaningful answers
- [ ] Consensus builds correctly
- [ ] UI displays responses smoothly
- [ ] Error handling works gracefully
- [ ] Performance < 60 seconds for 5 agents

---

## üìö Additional Resources

- [Railway Documentation](https://docs.railway.app/)
- [OpenAI API Reference](https://platform.openai.com/docs/api-reference)
- [LangGraph Docs](https://langchain-ai.github.io/langgraph/)
- [AutoGen Docs](https://microsoft.github.io/autogen/)
- [CrewAI Docs](https://docs.crewai.com/)

---

## üéâ You're Done!

Your Ask Panel is now fully integrated with Railway AI Engine!

**Next Steps:**
1. Test with real questions
2. Monitor performance
3. Gather user feedback
4. Optimize based on usage patterns

**Questions?** Check the troubleshooting section or Railway logs.

---

**Built with ‚ù§Ô∏è for Digital Health Innovation**

