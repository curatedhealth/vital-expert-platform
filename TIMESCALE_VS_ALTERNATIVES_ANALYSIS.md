# TimescaleDB vs Alternatives - RAG Metrics Storage Analysis

**Date**: October 27, 2025
**Context**: Phase 2 Long-term Metrics Storage Evaluation

---

## ðŸŽ¯ The Problem: Why We Need Long-term Storage

### Current Situation (In-Memory Storage)

**Monitoring services store metrics in-memory**:
```typescript
// rag-latency-tracker.ts
private metrics: LatencyMetrics[] = [];  // Last 10,000 operations (~2-3 hours at 50 queries/min)

// rag-cost-tracker.ts
private entries: CostEntry[] = [];  // Last 100,000 entries (~24 hours at 70 ops/min)
```

**Limitations**:
- âŒ Data lost on server restart
- âŒ Limited retention (2-3 hours latency, 24 hours cost)
- âŒ No historical analysis beyond 24 hours
- âŒ Can't compare week-over-week or month-over-month
- âŒ Can't identify long-term trends
- âŒ Can't do advanced analytics (SQL queries, aggregations)
- âŒ Memory consumption grows with usage

**What You Need for Production**:
- âœ… Historical metrics (weeks, months, years)
- âœ… Week-over-week comparisons
- âœ… Long-term trend analysis
- âœ… Cost forecasting
- âœ… Performance regression detection
- âœ… Compliance reporting (FDA requires audit trails)
- âœ… Advanced analytics (SQL, time-series queries)

---

## ðŸ“Š Solution Comparison

### Option 1: **TimescaleDB** (PostgreSQL Extension) â­ RECOMMENDED

**What is TimescaleDB?**
- Open-source time-series database
- PostgreSQL extension (100% SQL compatible)
- Automatic time-series optimization
- Built for metrics, events, and IoT data

**Why TimescaleDB for RAG Metrics?**

#### Advantages:

**1. Perfect Fit for Time-Series Data**
```sql
-- Store latency metrics with automatic partitioning
CREATE TABLE rag_latency_metrics (
    time TIMESTAMPTZ NOT NULL,
    query_id UUID,
    strategy TEXT,
    p95_latency_ms REAL,
    cache_hit BOOLEAN,
    user_id TEXT,
    agent_id TEXT
);

-- Convert to hypertable (TimescaleDB magic)
SELECT create_hypertable('rag_latency_metrics', 'time');

-- Automatic compression after 7 days (90% storage savings)
ALTER TABLE rag_latency_metrics SET (
    timescaledb.compress,
    timescaledb.compress_segmentby = 'strategy,user_id'
);
```

**2. Already Have PostgreSQL (Supabase)**
- âœ… You already use Supabase (PostgreSQL)
- âœ… No new infrastructure needed
- âœ… Can use existing database credentials
- âœ… Native integration with your stack
- âœ… Supabase supports TimescaleDB extension

**3. SQL Queries for Analytics**
```sql
-- Week-over-week P95 latency comparison
SELECT
    time_bucket('1 week', time) AS week,
    strategy,
    percentile_cont(0.95) WITHIN GROUP (ORDER BY p95_latency_ms) AS p95
FROM rag_latency_metrics
WHERE time > NOW() - INTERVAL '1 month'
GROUP BY week, strategy
ORDER BY week DESC;

-- Cost trends by user
SELECT
    time_bucket('1 day', time) AS day,
    user_id,
    SUM(cost_usd) AS daily_cost
FROM rag_cost_entries
WHERE time > NOW() - INTERVAL '30 days'
GROUP BY day, user_id
ORDER BY day DESC, daily_cost DESC;

-- Identify cost spikes
SELECT
    time,
    query_id,
    cost_usd,
    operation
FROM rag_cost_entries
WHERE cost_usd > (
    SELECT AVG(cost_usd) * 3 FROM rag_cost_entries
)
ORDER BY cost_usd DESC
LIMIT 10;
```

**4. Automatic Data Retention & Compression**
```sql
-- Keep raw data for 7 days, compress older data
SELECT add_compression_policy('rag_latency_metrics', INTERVAL '7 days');

-- Drop data older than 1 year
SELECT add_retention_policy('rag_latency_metrics', INTERVAL '1 year');
```

**5. Grafana Native Integration**
- TimescaleDB has first-class Grafana support
- Pre-built templates for metrics dashboards
- Real-time visualization
- Alerting built-in

**6. Cost-Effective**
```
Supabase with TimescaleDB:
- Free tier: Up to 500MB database (enough for millions of metrics)
- Pro tier: $25/month for 8GB (enough for years of metrics)
- Compression reduces storage by 90%

Storage estimate:
- 1 latency metric: ~200 bytes
- 10,000 metrics/day: 2MB/day uncompressed â†’ 200KB/day compressed
- 1 month: 6MB compressed
- 1 year: 72MB compressed âœ… (fits in free tier!)
```

**7. Production-Grade Features**
- âœ… ACID transactions (data consistency)
- âœ… Replication (high availability)
- âœ… Point-in-time recovery (backup/restore)
- âœ… Row-level security (multi-tenant support)
- âœ… Full-text search
- âœ… JSON support (flexible schema)

#### Implementation Complexity: **LOW** â­
```typescript
// 1. Enable TimescaleDB extension in Supabase (1 click)
// 2. Create hypertables (5 min)
// 3. Update monitoring services to write to DB (30 min)
// 4. Create Grafana dashboard (1 hour)

Total: ~2 hours implementation
```

#### Disadvantages:
- âš ï¸ Requires database migration (one-time setup)
- âš ï¸ Adds latency to writes (~10-20ms per metric)
- âš ï¸ Need to manage database schema

---

### Option 2: **InfluxDB** (Purpose-Built Time-Series DB)

**What is InfluxDB?**
- Dedicated time-series database
- High write throughput
- Built-in downsampling and retention

**Advantages**:
- âœ… Higher write performance than TimescaleDB
- âœ… Built-in data lifecycle management
- âœ… Native Grafana support
- âœ… Flux query language (powerful for time-series)
- âœ… Excellent for IoT and metrics use cases

**Disadvantages**:
- âŒ **Additional infrastructure** (new database service)
- âŒ **Additional cost** ($0.40/GB/month on InfluxDB Cloud)
- âŒ **Learning curve** (Flux query language, not SQL)
- âŒ **Not PostgreSQL** (can't leverage existing Supabase)
- âŒ **Vendor lock-in** (InfluxDB-specific)

**Cost Estimate**:
```
InfluxDB Cloud:
- Free tier: 30-day retention, 5MB/5min writes
- Pay-as-you-go: $0.40/GB/month storage + $0.002/GB writes

1 year of metrics:
- Storage: 72MB compressed Ã— $0.40/GB = $0.03/month
- Writes: ~2MB/day Ã— 365 = 730MB Ã— $0.002 = $1.46/year
Total: ~$1.80/year

BUT: Requires separate service, ops overhead
```

**Implementation Complexity**: **MEDIUM**
```
1. Sign up for InfluxDB Cloud (15 min)
2. Configure InfluxDB client in app (30 min)
3. Update monitoring services (30 min)
4. Create Grafana dashboard (1 hour)

Total: ~2.5 hours + ongoing ops
```

---

### Option 3: **Prometheus** (Metrics Aggregation)

**What is Prometheus?**
- Open-source monitoring system
- Pull-based metrics collection
- Designed for infrastructure monitoring

**Advantages**:
- âœ… Industry standard for metrics
- âœ… Excellent Grafana integration
- âœ… Powerful query language (PromQL)
- âœ… Service discovery
- âœ… Alerting built-in

**Disadvantages**:
- âŒ **Not designed for long-term storage** (90 days default)
- âŒ **Requires separate long-term storage** (Thanos, Cortex)
- âŒ **Pull-based** (app exposes /metrics endpoint, Prometheus scrapes)
- âŒ **Not ideal for per-query granularity** (aggregate metrics only)
- âŒ **Cardinality limitations** (struggles with high-cardinality labels like query_id)
- âŒ **Additional infrastructure** (Prometheus server + Thanos/Cortex)

**Cost Estimate**:
```
Self-hosted Prometheus + Thanos:
- Server cost: $10-20/month (1GB RAM + 20GB storage)
- Thanos object storage: $0.023/GB/month (S3)
- 1 year: ~$5 S3 + $240 server = $245/year

OR Grafana Cloud with Prometheus:
- Free tier: 10K metrics, 14-day retention
- Pro tier: $50/month for 15K metrics + long-term storage
Total: $600/year
```

**Implementation Complexity**: **HIGH**
```
1. Set up Prometheus server (1 hour)
2. Configure /metrics endpoint in app (1 hour)
3. Set up Thanos for long-term storage (2 hours)
4. Configure Grafana (1 hour)

Total: ~5 hours + complex ops
```

---

### Option 4: **Elasticsearch** (Search + Analytics)

**What is Elasticsearch?**
- Distributed search and analytics engine
- JSON document store
- Built for log aggregation

**Advantages**:
- âœ… Powerful full-text search
- âœ… Excellent for log analysis
- âœ… Kibana for visualization
- âœ… Aggregations for analytics

**Disadvantages**:
- âŒ **Overkill for metrics** (designed for logs, not time-series)
- âŒ **High resource usage** (Java-based, memory-hungry)
- âŒ **Expensive** ($90/month minimum on Elastic Cloud)
- âŒ **Complex operations** (cluster management, sharding)
- âŒ **Not optimized for time-series queries**

**Cost Estimate**:
```
Elastic Cloud:
- Standard tier: $90/month (4GB RAM, 120GB storage)
- Premium tier: $190/month
Total: $1,080-2,280/year

Self-hosted:
- Server: $40/month (8GB RAM + 200GB storage)
Total: $480/year + ops overhead
```

**Implementation Complexity**: **VERY HIGH**
```
1. Set up Elasticsearch cluster (3 hours)
2. Configure index templates (1 hour)
3. Update monitoring services (1 hour)
4. Set up Kibana (1 hour)

Total: ~6 hours + complex ops
```

---

### Option 5: **Supabase Only** (PostgreSQL + pg_cron)

**What is this?**
- Use existing Supabase PostgreSQL
- WITHOUT TimescaleDB extension
- Standard tables with indexes

**Advantages**:
- âœ… **No additional infrastructure**
- âœ… Already have it (Supabase)
- âœ… SQL queries (familiar)
- âœ… Free tier sufficient

**Disadvantages**:
- âŒ **Not optimized for time-series** (slower queries)
- âŒ **No automatic compression** (higher storage costs)
- âŒ **Manual partitioning required** (complex)
- âŒ **Slower aggregations** (no time_bucket function)
- âŒ **Index bloat over time** (performance degrades)

**Why Not Recommended**:
```sql
-- Without TimescaleDB
SELECT
    DATE_TRUNC('hour', timestamp) AS hour,
    AVG(latency_ms) AS avg_latency
FROM rag_metrics
WHERE timestamp > NOW() - INTERVAL '30 days'
GROUP BY hour
ORDER BY hour;

-- Slower query (no automatic partitioning)
-- Larger storage (no compression)
-- Manual maintenance (VACUUM, REINDEX)
```

**Cost**: Same as TimescaleDB ($0 for free tier), but worse performance

**Implementation Complexity**: **LOW** (but worse outcomes)

---

## ðŸ“Š Decision Matrix

| Feature | TimescaleDB | InfluxDB | Prometheus | Elasticsearch | Supabase Only |
|---------|-------------|----------|------------|---------------|---------------|
| **Time-Series Optimized** | âœ… Excellent | âœ… Excellent | âœ… Good | âš ï¸ Basic | âŒ Poor |
| **SQL Compatible** | âœ… Yes | âŒ No (Flux) | âŒ No (PromQL) | âŒ No (JSON) | âœ… Yes |
| **Existing Infrastructure** | âœ… Use Supabase | âŒ New service | âŒ New service | âŒ New service | âœ… Have it |
| **Cost (1 year)** | **$0-25** | ~$20 | ~$240 | ~$1,080 | $0-25 |
| **Implementation Time** | **2 hours** | 2.5 hours | 5 hours | 6 hours | 1 hour |
| **Ops Complexity** | â­ Low | â­â­ Medium | â­â­â­ High | â­â­â­â­ Very High | â­ Low |
| **Grafana Integration** | âœ… Native | âœ… Native | âœ… Native | âš ï¸ Kibana | âœ… Native |
| **Automatic Compression** | âœ… 90% savings | âœ… Built-in | âŒ No | âš ï¸ Manual | âŒ No |
| **Long-term Retention** | âœ… Years | âœ… Years | âš ï¸ 90 days | âœ… Years | âœ… Years |
| **Per-Query Granularity** | âœ… Perfect | âœ… Perfect | âŒ Limited | âœ… Good | âœ… Perfect |
| **Multi-tenant Support** | âœ… RLS | âš ï¸ Manual | âš ï¸ Labels | âš ï¸ Manual | âœ… RLS |
| **Compliance (FDA)** | âœ… ACID | âš ï¸ Eventually consistent | âš ï¸ Not designed | âš ï¸ Not designed | âœ… ACID |

---

## ðŸŽ¯ Recommendation: **TimescaleDB** â­

### Why TimescaleDB is the Clear Winner:

**1. Leverage Existing Infrastructure**
- âœ… You already have Supabase (PostgreSQL)
- âœ… No new services to manage
- âœ… Use existing database credentials
- âœ… Same backup/restore procedures
- âœ… Same monitoring tools

**2. Cost-Effective**
```
Year 1: $0 (free tier covers ~1 million metrics)
Year 2+: $25/month if you exceed 8GB (unlikely with compression)

vs InfluxDB: $20/year
vs Prometheus: $240/year
vs Elasticsearch: $1,080/year
```

**3. SQL = Familiar & Powerful**
```sql
-- Week-over-week P95 comparison (one query!)
SELECT
    time_bucket('1 week', time) AS week,
    percentile_cont(0.95) WITHIN GROUP (ORDER BY p95_latency_ms) AS p95,
    LAG(percentile_cont(0.95) WITHIN GROUP (ORDER BY p95_latency_ms))
        OVER (ORDER BY time_bucket('1 week', time)) AS prev_week_p95
FROM rag_latency_metrics
WHERE time > NOW() - INTERVAL '2 months'
GROUP BY week
ORDER BY week DESC;

-- Find cost anomalies
WITH baseline AS (
    SELECT AVG(cost_usd) AS avg_cost, STDDEV(cost_usd) AS std_cost
    FROM rag_cost_entries
    WHERE time > NOW() - INTERVAL '7 days'
)
SELECT * FROM rag_cost_entries
WHERE cost_usd > (SELECT avg_cost + 3 * std_cost FROM baseline)
ORDER BY time DESC;
```

**4. Perfect for Healthcare Compliance**
- âœ… ACID transactions (FDA requires data integrity)
- âœ… Point-in-time recovery (audit trail requirements)
- âœ… Row-level security (HIPAA multi-tenant)
- âœ… Immutable audit logs
- âœ… SQL for regulatory reporting

**5. Low Operations Overhead**
- âœ… Managed by Supabase (no servers to maintain)
- âœ… Automatic backups
- âœ… Automatic compression (90% storage savings)
- âœ… Automatic retention policies
- âœ… No cluster management

**6. Grafana Integration**
- âœ… Native PostgreSQL data source
- âœ… Pre-built TimescaleDB dashboards
- âœ… Time-series functions (time_bucket, etc.)
- âœ… Alerting on SQL queries

---

## ðŸš€ Implementation Roadmap (2 hours)

### Phase 2A: TimescaleDB Setup (30 minutes)

**Step 1: Enable Extension in Supabase** (5 min)
```sql
-- Run in Supabase SQL editor
CREATE EXTENSION IF NOT EXISTS timescaledb CASCADE;
```

**Step 2: Create Hypertables** (10 min)
```sql
-- Latency metrics table
CREATE TABLE rag_latency_metrics (
    time TIMESTAMPTZ NOT NULL,
    query_id UUID NOT NULL,
    strategy TEXT NOT NULL,
    cache_hit BOOLEAN NOT NULL,
    cache_check_ms REAL,
    query_embedding_ms REAL,
    vector_search_ms REAL,
    reranking_ms REAL,
    total_retrieval_ms REAL NOT NULL,
    user_id TEXT,
    agent_id TEXT,
    session_id TEXT,
    tenant_id TEXT
);

SELECT create_hypertable('rag_latency_metrics', 'time');

-- Cost entries table
CREATE TABLE rag_cost_entries (
    time TIMESTAMPTZ NOT NULL,
    query_id UUID NOT NULL,
    operation TEXT NOT NULL,
    provider TEXT NOT NULL,
    model TEXT NOT NULL,
    token_count INTEGER,
    vector_count INTEGER,
    cost_usd REAL NOT NULL,
    user_id TEXT,
    agent_id TEXT,
    session_id TEXT,
    tenant_id TEXT
);

SELECT create_hypertable('rag_cost_entries', 'time');

-- Circuit breaker events table
CREATE TABLE rag_circuit_breaker_events (
    time TIMESTAMPTZ NOT NULL,
    service_name TEXT NOT NULL,
    event_type TEXT NOT NULL, -- 'opened', 'closed', 'half_open', 'rejected'
    failures INTEGER,
    successes INTEGER,
    tenant_id TEXT
);

SELECT create_hypertable('rag_circuit_breaker_events', 'time');
```

**Step 3: Configure Compression & Retention** (5 min)
```sql
-- Compress data older than 7 days (90% storage savings)
SELECT add_compression_policy('rag_latency_metrics', INTERVAL '7 days');
SELECT add_compression_policy('rag_cost_entries', INTERVAL '7 days');

-- Keep data for 1 year, drop older
SELECT add_retention_policy('rag_latency_metrics', INTERVAL '1 year');
SELECT add_retention_policy('rag_cost_entries', INTERVAL '1 year');
SELECT add_retention_policy('rag_circuit_breaker_events', INTERVAL '90 days');
```

**Step 4: Create Indexes** (10 min)
```sql
-- Indexes for common queries
CREATE INDEX idx_latency_user_time ON rag_latency_metrics (user_id, time DESC);
CREATE INDEX idx_latency_agent_time ON rag_latency_metrics (agent_id, time DESC);
CREATE INDEX idx_latency_strategy ON rag_latency_metrics (strategy, time DESC);

CREATE INDEX idx_cost_user_time ON rag_cost_entries (user_id, time DESC);
CREATE INDEX idx_cost_operation ON rag_cost_entries (operation, time DESC);
CREATE INDEX idx_cost_provider ON rag_cost_entries (provider, time DESC);
```

### Phase 2B: Update Monitoring Services (1 hour)

**Create database adapter** (30 min):
```typescript
// src/lib/services/monitoring/timescale-adapter.ts
import { createClient } from '@supabase/supabase-js';

export class TimescaleAdapter {
  private supabase = createClient(
    process.env.NEXT_PUBLIC_SUPABASE_URL!,
    process.env.SUPABASE_SERVICE_ROLE_KEY!
  );

  async insertLatencyMetric(metric: LatencyMetrics) {
    await this.supabase.from('rag_latency_metrics').insert({
      time: metric.timestamp,
      query_id: metric.queryId,
      strategy: metric.strategy,
      cache_hit: metric.cacheHit,
      // ... other fields
    });
  }

  async insertCostEntry(entry: CostEntry) {
    await this.supabase.from('rag_cost_entries').insert({
      time: entry.timestamp,
      query_id: entry.queryId,
      // ... other fields
    });
  }

  // Query methods for dashboard
  async getLatencyBreakdown(windowMinutes: number) {
    const { data } = await this.supabase.rpc('get_latency_breakdown', {
      window_minutes: windowMinutes
    });
    return data;
  }
}
```

**Update trackers to write to DB** (30 min):
```typescript
// Modify rag-latency-tracker.ts
trackOperation(metrics: LatencyMetrics): void {
  // Existing in-memory storage
  this.metrics.push(metrics);

  // NEW: Async write to TimescaleDB (non-blocking)
  if (process.env.ENABLE_TIMESCALE_STORAGE === 'true') {
    this.timescaleAdapter.insertLatencyMetric(metrics)
      .catch(err => console.error('TimescaleDB write failed:', err));
  }
}
```

### Phase 2C: Grafana Dashboard (30 minutes)

**Create dashboards** (templates provided in Phase 2 implementation)

---

## ðŸ’° Cost Comparison (3-Year TCO)

| Solution | Year 1 | Year 2 | Year 3 | 3-Year Total | Ops Effort |
|----------|--------|--------|--------|--------------|------------|
| **TimescaleDB** | **$0** | **$0** | **$0** | **$0** | â­ Minimal |
| InfluxDB | $20 | $20 | $20 | $60 | â­â­ Low |
| Prometheus | $240 | $240 | $240 | $720 | â­â­â­ Medium |
| Elasticsearch | $1,080 | $1,080 | $1,080 | $3,240 | â­â­â­â­ High |
| Supabase Only | $0 | $0 | $0 | $0 | â­â­ Low (but poor performance) |

**Winner**: TimescaleDB ($0 for years with excellent performance)

---

## ðŸŽ¯ Conclusion

**Recommendation**: Use **TimescaleDB** â­

**Why**:
1. âœ… **Free** (Supabase free tier covers years of metrics)
2. âœ… **Fast** (optimized for time-series queries)
3. âœ… **Familiar** (SQL, not a new query language)
4. âœ… **Integrated** (already have PostgreSQL/Supabase)
5. âœ… **Low ops** (managed, automatic compression/retention)
6. âœ… **Compliant** (ACID, audit trails, RLS)
7. âœ… **Quick to implement** (2 hours total)

**When to Reconsider**:
- If you have >1M metrics/day (consider InfluxDB for higher write throughput)
- If you need <1ms write latency (consider in-memory + async batch writes)
- If you already use Prometheus for infrastructure (add Thanos for long-term)

**For VITAL's use case** (healthcare AI with compliance requirements):
**TimescaleDB is the perfect fit.**

---

**Analysis Created**: October 27, 2025
**Recommendation**: TimescaleDB (PostgreSQL extension via Supabase)
**Cost**: $0 (free tier sufficient)
**Implementation**: 2 hours
**Complexity**: LOW â­
