# ğŸš€ LangGraph Knowledge Pipeline - Advanced Document Processing

## Overview

The Knowledge Pipeline now features an **advanced LangGraph-based workflow** that provides comprehensive, multi-stage document processing with intelligent routing and quality validation.

---

## ğŸ¯ Architecture

### Traditional Approach (Before)
```
Scrape â†’ Upload â†’ Done
```

### New LangGraph Approach (Now)
```
Scrape â†’ Metadata Enrichment â†’ Quality Validation â†’ Chunking â†’ 
Embeddings â†’ Supabase Storage â†’ Pinecone Upload â†’ Finalization
```

---

## ğŸ“Š LangGraph Workflow Stages

### 1. **Metadata Enrichment** ğŸ“
- Maps scraped data to comprehensive metadata schema (85+ fields)
- Auto-calculates quality scores (quality, credibility, freshness)
- Extracts word count and content statistics
- **Decision Point**: Continue if content exists and word count > 50

### 2. **Quality Validation** ğŸ”
- Validates quality score (threshold: 2.0/10)
- Checks minimum word count (100 words)
- Flags warnings for low quality or short content
- **Decision Point**: Continue if quality > 2.0

### 3. **Document Chunking** âœ‚ï¸
- Uses `RecursiveCharacterTextSplitter` from LangChain
- Chunk size: 1,000 characters
- Chunk overlap: 200 characters
- Smart separators: `\n\n`, `\n`, `. `, `! `, `? `, ` `

### 4. **Embedding Generation** ğŸ§ 
- Generates vector embeddings for each chunk
- Uses configured embedding model (HuggingFace or OpenAI)
- Leverages Unified RAG Service's embedding layer
- Batch processing for efficiency

### 5. **Supabase Storage** ğŸ’¾
- Stores full document content and metadata
- Creates or updates in `knowledge_documents` table
- Manages domain associations (`knowledge_domains_new`)
- Handles duplicate detection (by URL)

### 6. **Pinecone Upload** ğŸ“¤
- Uploads vector embeddings to Pinecone
- Namespace routing based on domain
- Batch uploads (100 vectors per batch)
- Includes chunk metadata for retrieval

### 7. **Finalization** ğŸ
- Validates overall success
- Compiles errors and warnings
- Returns comprehensive result state

---

## ğŸ”„ State Management

The workflow uses a `DocumentProcessingState` TypedDict:

```python
{
    # Input
    'raw_content': str,
    'source_url': str,
    'source_metadata': dict,
    
    # Processing stages
    'enriched_metadata': dict,
    'chunks': list[str],
    'embeddings': list[list[float]],
    'document_id': str,
    'domain_id': str,
    
    # Status tracking
    'stage': str,  # Current stage
    'errors': list[str],
    'warnings': list[str],
    
    # Statistics
    'word_count': int,
    'chunk_count': int,
    'quality_score': float,
    
    # Results
    'success': bool,
    'pinecone_vectors_uploaded': int,
    'supabase_stored': bool
}
```

---

## ğŸŒŸ Key Features

### 1. **Intelligent Routing**
- Conditional edges based on quality and content
- Automatic skip of low-quality documents
- Graceful error handling at each stage

### 2. **Comprehensive Metadata**
- 85+ metadata fields
- Auto-calculated scores:
  - **Quality Score**: Content depth, structure, references
  - **Credibility Score**: Source reputation, citations
  - **Freshness Score**: Publication date, updates
  - **Readability Score**: Flesch-Kincaid grade level
  - **Technical Complexity**: Specialized terminology density

### 3. **Integration with Existing Services**
- **Unified RAG Service**: Core processing engine
- **Supabase Client**: Database operations
- **Cache Manager**: Performance optimization
- **Embedding Service Factory**: Multi-provider support

### 4. **Fallback Mechanism**
- Attempts LangGraph workflow first
- Falls back to standard RAG integration if unavailable
- No breaking changes to existing pipelines

---

## ğŸ“ Usage Examples

### Basic Usage (Automatic with Pipeline)

When you run the knowledge pipeline with "Run All", it automatically uses the LangGraph workflow:

```bash
cd scripts
python3 knowledge-pipeline.py --config your-config.json
```

**Output:**
```
âœ… LangGraph processor initialized - using advanced workflow ğŸš€
ğŸ“‹ Workflow stages: metadata enrichment â†’ validation â†’ chunking â†’ embeddings â†’ storage

ğŸ”„ Processing with LangGraph workflow: Sharing Digital Health Educational...

ğŸ“ Stage 1: Enriching metadata
âœ… Metadata enriched - Quality: 5.05, Words: 9892

ğŸ” Stage 2: Validating document quality
âœ… Quality validation complete - Score: 5.05

âœ‚ï¸ Stage 3: Chunking document
âœ… Created 12 chunks

ğŸ§  Stage 4: Generating embeddings for 12 chunks
âœ… Generated 12 embeddings

ğŸ’¾ Stage 5: Storing in Supabase
âœ… Stored in Supabase - Document ID: abc123

ğŸ“¤ Stage 6: Uploading to Pinecone
âœ… Uploaded 12 vectors to Pinecone

ğŸ Stage 7: Finalizing
ğŸ‰ Processing complete - Success!

âœ… LangGraph processing complete:
   ğŸ“Š Quality Score: 5.05
   âœ‚ï¸ Chunks: 12
   ğŸ“¤ Vectors uploaded: 12
   ğŸ’¾ Supabase: âœ…
```

### Programmatic Usage

```python
from services.knowledge_pipeline_langgraph import create_knowledge_processor

# Initialize processor
processor = await create_knowledge_processor(
    embedding_model="sentence-transformers/all-MiniLM-L6-v2"
)

# Process a single document
result = await processor.process_document(
    raw_content="Your document text here...",
    source_url="https://example.com/article",
    source_metadata={
        'title': 'Example Article',
        'domain': 'healthcare',
        'category': 'research',
        'tags': ['digital-health', 'AI'],
        'firm': 'PubMed Central / NIH'
    }
)

# Check results
if result['success']:
    print(f"âœ… Success!")
    print(f"   Quality: {result['quality_score']:.2f}")
    print(f"   Chunks: {result['chunk_count']}")
    print(f"   Vectors: {result['pinecone_vectors_uploaded']}")
else:
    print(f"âŒ Failed: {result['errors']}")
```

### Batch Processing

```python
# Process multiple documents concurrently
documents = [
    {
        'content': 'Document 1 text...',
        'url': 'https://example.com/doc1',
        'metadata': {...}
    },
    {
        'content': 'Document 2 text...',
        'url': 'https://example.com/doc2',
        'metadata': {...}
    }
]

results = await processor.process_batch(
    documents=documents,
    max_concurrent=3  # Process 3 at a time
)

# Analyze results
success_count = sum(1 for r in results if r['success'])
print(f"Processed: {len(results)}, Success: {success_count}")
```

---

## ğŸ”§ Configuration

### Environment Variables

The LangGraph workflow uses the same environment variables as the existing system:

```bash
# Supabase (required)
SUPABASE_URL=your-supabase-url
SUPABASE_SERVICE_ROLE_KEY=your-service-role-key

# Pinecone (required for vector search)
PINECONE_API_KEY=your-pinecone-api-key
PINECONE_INDEX_NAME=vital-knowledge

# Embeddings (optional - defaults to HuggingFace)
EMBEDDING_PROVIDER=huggingface  # or 'openai'
OPENAI_API_KEY=your-openai-key  # if using OpenAI
```

### Chunking Parameters

Customize chunking in `knowledge_pipeline_langgraph.py`:

```python
self.text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,        # Adjust chunk size
    chunk_overlap=200,      # Adjust overlap
    length_function=len,
    separators=["\n\n", "\n", ". ", "! ", "? ", " ", ""]
)
```

### Quality Thresholds

Adjust quality validation thresholds:

```python
def _should_continue_after_validation(self, state):
    if state['quality_score'] < 2.0:  # Adjust threshold
        return "skip"
    return "chunk"
```

---

## ğŸ“ˆ Performance Metrics

### Processing Speed

| Stage | Time (per document) |
|-------|---------------------|
| Metadata Enrichment | ~0.1s |
| Quality Validation | ~0.01s |
| Chunking | ~0.05s |
| Embedding Generation | ~1-3s |
| Supabase Storage | ~0.2s |
| Pinecone Upload | ~0.5s |
| **Total** | **~2-4s** |

### Resource Usage

- **Memory**: ~100-200 MB per document
- **CPU**: Moderate (embedding generation)
- **Network**: Batch uploads minimize requests

---

## ğŸ¨ Workflow Visualization

```mermaid
graph TD
    A[Start: Raw Content] --> B[Enrich Metadata]
    B --> C{Content Valid?}
    C -->|Yes| D[Validate Quality]
    C -->|No| Z[Skip/End]
    D --> E{Quality > 2.0?}
    E -->|Yes| F[Chunk Document]
    E -->|No| Z
    F --> G[Generate Embeddings]
    G --> H[Store in Supabase]
    H --> I[Upload to Pinecone]
    I --> J[Finalize]
    J --> K[End: Success!]
```

---

## ğŸ› Troubleshooting

### LangGraph Not Available

**Error:** `âš ï¸ LangGraph not available: No module named 'langgraph'`

**Solution:**
```bash
pip install langgraph langchain-core langchain-text-splitters
```

### Low Quality Documents Skipped

**Warning:** `âš ï¸ Skipping low quality document (score: 1.5)`

**Solution:**
- Lower the quality threshold in `_should_continue_after_validation`
- Or improve source selection to get higher quality content

### Embedding Generation Failed

**Error:** `âŒ Failed to generate embeddings`

**Solution:**
- Check embedding service configuration
- Verify API keys (OpenAI) or model availability (HuggingFace)
- Check network connectivity

### Pinecone Upload Failed

**Error:** `âŒ Error uploading to Pinecone`

**Solution:**
- Verify `PINECONE_API_KEY` is set
- Check index name is correct
- Ensure namespace routing is working

---

## ğŸ”„ Migration Guide

### From Standard RAG Integration

No changes needed! The system automatically:
1. Tries to use LangGraph workflow
2. Falls back to standard integration if unavailable
3. Maintains backward compatibility

### Disabling LangGraph

If you want to force standard integration:

In `knowledge-pipeline.py`:
```python
class RAGServiceUploader:
    def __init__(self, embedding_model: str):
        ...
        self.use_langgraph = False  # Force standard mode
```

---

## ğŸ“š Related Files

- **LangGraph Processor**: `services/ai-engine/src/services/knowledge_pipeline_langgraph.py`
- **Standard RAG Integration**: `services/ai-engine/src/services/knowledge_pipeline_integration.py`
- **Unified RAG Service**: `services/ai-engine/src/services/unified_rag_service.py`
- **Main Pipeline**: `scripts/knowledge-pipeline.py`
- **Metadata Calculator**: `scripts/metadata_auto_calculator.py`
- **Metadata Mapper**: `scripts/comprehensive_metadata_mapper.py`

---

## ğŸ¯ Benefits

### 1. **Quality Assurance**
- Multi-stage validation
- Automatic filtering of low-quality content
- Detailed error and warning tracking

### 2. **Comprehensive Metadata**
- 85+ metadata fields automatically populated
- Quality scores calculated
- Rich context for RAG retrieval

### 3. **Intelligent Processing**
- Conditional routing based on content quality
- Resource optimization (skip bad documents early)
- Graceful degradation with fallbacks

### 4. **Observability**
- Detailed logging at each stage
- Error tracking and reporting
- Performance metrics

### 5. **Scalability**
- Batch processing support
- Concurrent document handling
- Efficient resource usage

---

## ğŸ‰ Summary

The LangGraph-based Knowledge Pipeline provides:

âœ… **7-stage processing workflow**  
âœ… **Comprehensive metadata enrichment**  
âœ… **Quality validation and filtering**  
âœ… **Intelligent chunking and embedding**  
âœ… **Dual storage (Supabase + Pinecone)**  
âœ… **Detailed logging and error tracking**  
âœ… **Backward compatible with existing system**  
âœ… **Production-ready with fallback mechanisms**  

**Ready to use! Just run the pipeline and watch the advanced workflow in action! ğŸš€**

